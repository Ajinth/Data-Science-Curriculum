{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder is an artificial neural network used for unsupervised learning of efficient codings. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. Recently, the autoencoder concept has become more widely used for learning generative models of data.\n",
    "\n",
    "Architecturally, the simplest form of an autoencoder is a feedforward, non-recurrent neural network very similar to the multilayer perceptron (MLP) â€“ having an input layer, an output layer, and one or more hidden layers connecting them, but with the output layer having the same number of nodes as the input layer, and with the purpose of reconstructing its own inputs (instead of predicting the target value _Y_ given inputs _X_. Therefore, autoencoders are unsupervised learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our basic representation of an autoencoder with one hidden layer can be expressed as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\ Z = s(X.dot(W_h)+b_h)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\ X_{hat} = s(Z.dot(W_o)+b_o)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, a common modification that we make in order to simplify the equation and introduce a degree of regularization is to use the transpose of $W_h$ on the output layer. This is known as using _shared weights_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\\begin{align}\n",
    "\\ Z = s(X.dot(W)+b_h)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\\begin{align}\n",
    "\\ X_{hat} = s(Z.dot(W^T)+b_o)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function in autoencoders is: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\ J = \\mid X - X_{hat} \\mid_F^2 = \\mid X - s(s(XW)W^T) \\mid_F^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is _obvious_ from the equation above, autoencoders are like nonlinear PCA. The primary differences are: \n",
    "- bias\n",
    "- requirement that column of W has length of 1\n",
    "- orthogonality\n",
    "- order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoising autoencoders add noise to our input in order to help our data get better at reconstruction the original input. The denoising auto-encoder is a stochastic version of the auto-encoder. In one variation, the stochastic corruption process randomly sets some of the inputs (as many as half of them) to zero. Hence the denoising auto-encoder is trying to predict the corrupted (i.e. missing) values from the uncorrupted (i.e., non-missing) values, for randomly selected subsets of missing patterns. Note how being able to predict any subset of variables from the rest is a sufficient condition for completely capturing the joint distribution between a set of variables (this is how Gibbs sampling works)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacked autoencoders work like this: \n",
    "- start with the input data X\n",
    "- train an autoencoder on X; the hidden layer's output is $Z_1$\n",
    "- train an autoencoder on $Z_1$ (target is $Z_1$); the hidden layer's output is $Z_2$\n",
    "- repeat  \n",
    "\n",
    "We want each layer to be a more compact representation than the last, so each layer will decrease in size. This is one way we can use stacked autoencoders for data compression. This is not a requirement, though, and even using as many hidden units as there are observations has been shown to not lead to overfitting.  \n",
    "\n",
    "The process is know as _greedy layer-wise pretraining_. It means that we are making the best short-sighted decision, and it works well to use as a precursor to supervised learning. But the good news is that we don't have to do too many epochs of backpropagation, since the autoencoder does most of the work on this front. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "os.chdir('/home/z/Dropbox/Documents/Analytics/Learning/Data-Science-Curriculum/extras')\n",
    "from util import relu, error_rate, getKaggleMNIST, init_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(object):\n",
    "    # M is an arbitrary parameter\n",
    "    # ID is used for setting names of theano variables\n",
    "    def __init__(self, M, an_id):\n",
    "        self.M = M\n",
    "        self.id = an_id\n",
    "\n",
    "    # we will include momentum in our autoencoder'\n",
    "    # no Y - unsupervised\n",
    "    def fit(self, X, learning_rate=0.5, mu=0.99, epochs=1, batch_sz=100, show_fig=False):\n",
    "        N, D = X.shape\n",
    "        # get number of batches\n",
    "        n_batches = N / batch_sz\n",
    "\n",
    "        #our weights will be D-by-M\n",
    "        W0 = init_weights((D, self.M))\n",
    "\n",
    "        # this is where we use the ID variable\n",
    "        self.W = theano.shared(W0, 'W_%s' % self.id)\n",
    "        self.bh = theano.shared(np.zeros(self.M), 'bh_%s' % self.id)\n",
    "        self.bo = theano.shared(np.zeros(D), 'bo_%s' % self.id)\n",
    "        \n",
    "        # we will keep all of our parameters b/c we will use them during gradient descent\n",
    "        self.params = [self.W, self.bh, self.bo]\n",
    "        \n",
    "        # we will keep the forward params for the deep NN class\n",
    "        self.forward_params = [self.W, self.bh]\n",
    "\n",
    "        # keep track of the changes of each variable because we will use momentum\n",
    "        # TODO: technically these should be reset before doing backprop\n",
    "        self.dW = theano.shared(np.zeros(W0.shape), 'dW_%s' % self.id)\n",
    "        self.dbh = theano.shared(np.zeros(self.M), 'dbh_%s' % self.id)\n",
    "        self.dbo = theano.shared(np.zeros(D), 'dbo_%s' % self.id)\n",
    "        \n",
    "        # similarly, collect the parameters here\n",
    "        self.dparams = [self.dW, self.dbh, self.dbo]\n",
    "        self.forward_dparams = [self.dW, self.dbh]\n",
    "\n",
    "        # now we define the tensor input, which is a matrix\n",
    "        X_in = T.matrix('X_%s' % self.id)\n",
    "        \n",
    "        # define x-hat, which is our reconstruction\n",
    "        X_hat = self.forward_output(X_in)\n",
    "\n",
    "        # attach it to the object so it can be used later\n",
    "        # must be sigmoidal because the output is also a sigmoid\n",
    "        # hidden layer operation will be defined as a theano function, since it will be used in the DNN\n",
    "        H = T.nnet.sigmoid(X_in.dot(self.W) + self.bh)\n",
    "        self.hidden_op = theano.function(\n",
    "            inputs=[X_in],\n",
    "            outputs=H,\n",
    "        )\n",
    "\n",
    "        # squared error cost function:\n",
    "        # cost = ((X_in - X_hat) * (X_in - X_hat)).sum() / N\n",
    "        \n",
    "        # cross-entropy cost function:\n",
    "        cost = -(X_in * T.log(X_hat) + (1 - X_in) * T.log(1 - X_hat)).sum() / (batch_sz * D)\n",
    "        cost_op = theano.function(\n",
    "            inputs=[X_in],\n",
    "            outputs=cost,\n",
    "        )\n",
    "\n",
    "        # this is our gradient descent (where we define our updates)\n",
    "        updates = [\n",
    "            (p, p + mu*dp - learning_rate*T.grad(cost, p)) for p, dp in zip(self.params, self.dparams)\n",
    "        ] + [\n",
    "            (dp, mu*dp - learning_rate*T.grad(cost, p)) for p, dp in zip(self.params, self.dparams)\n",
    "        ]\n",
    "        # here is our train function for the gradient descent\n",
    "        train_op = theano.function(\n",
    "            inputs=[X_in],\n",
    "            updates=updates,\n",
    "        )\n",
    "\n",
    "        # now we run our loop through this\n",
    "        costs = []\n",
    "        print(\"training autoencoder: %s\" % self.id)\n",
    "        for i in range(epochs):\n",
    "            print(\"epoch:\", i)\n",
    "            X = shuffle(X)\n",
    "            for j in range(int(n_batches)):\n",
    "                batch = X[j*batch_sz:(j*batch_sz + batch_sz)]\n",
    "                train_op(batch)\n",
    "                the_cost = cost_op(X) # technically we could also get the cost for Xtest here\n",
    "                if j == 408:\n",
    "                    print(\"j / n_batches:\", j, \"/\", n_batches, \"cost:\", the_cost)\n",
    "                costs.append(the_cost)\n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "\n",
    "    # this function will go to the hidden output, which takes in x\n",
    "    def forward_hidden(self, X):\n",
    "        Z = T.nnet.sigmoid(X.dot(self.W) + self.bh)\n",
    "        # Z = T.tanh(X.dot(self.W) + self.bh)\n",
    "        # Z = relu(X.dot(self.W) + self.bh)\n",
    "        return Z\n",
    "\n",
    "    # forward output will call the previous function and apply the sigmoid\n",
    "    def forward_output(self, X):\n",
    "        Z = self.forward_hidden(X)\n",
    "        # we transpose W because we are doing shared weights\n",
    "        Y = T.nnet.sigmoid(Z.dot(self.W.T) + self.bo)\n",
    "        return Y\n",
    "\n",
    "    @staticmethod\n",
    "    def createFromArrays(W, bh, bo, an_id):\n",
    "        ae = AutoEncoder(W.shape[1], an_id)\n",
    "        ae.W = theano.shared(W, 'W_%s' % ae.id)\n",
    "        ae.bh = theano.shared(bh, 'bh_%s' % ae.id)\n",
    "        ae.bo = theano.shared(bo, 'bo_%s' % ae.id)\n",
    "        ae.params = [ae.W, ae.bh, ae.bo]\n",
    "        ae.forward_params = [ae.W, ae.bh]\n",
    "        return ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DNN(object):\n",
    "\t# we will expand this later to accommodate RBMs as well\n",
    "    def __init__(self, hidden_layer_sizes, UnsupervisedModel=AutoEncoder):\n",
    "        self.hidden_layers = []\n",
    "        count = 0\n",
    "        # the count is the ID for each autoencoder\n",
    "        for M in hidden_layer_sizes:\n",
    "            ae = UnsupervisedModel(M, count)\n",
    "            self.hidden_layers.append(ae)\n",
    "            count += 1\n",
    "\n",
    "    # this is supervised, so it includes Y.\n",
    "    def fit(self, X, Y, Xtest, Ytest, pretrain=True, learning_rate=0.01, mu=0.99, reg=0.1, epochs=1, batch_sz=100):\n",
    "        # greedy layer-wise training of autoencoders\n",
    "        pretrain_epochs = 1\n",
    "        if not pretrain:\n",
    "            pretrain_epochs = 0\n",
    "\n",
    "        # this is where we do our pretrain\n",
    "        current_input = X\n",
    "        for ae in self.hidden_layers:\n",
    "            ae.fit(current_input, epochs=pretrain_epochs)\n",
    "\n",
    "            # create current_input for the next layer\n",
    "            current_input = ae.hidden_op(current_input)\n",
    "\n",
    "        # initialize logistic regression layer\n",
    "        N = len(Y)\n",
    "        # K is the number of the classes\n",
    "        K = len(set(Y))\n",
    "        # the size of the weight is the last hidden layer's M, and then K\n",
    "        W0 = init_weights((self.hidden_layers[-1].M, K))\n",
    "        self.W = theano.shared(W0, \"W_logreg\")\n",
    "        self.b = theano.shared(np.zeros(K), \"b_logreg\")\n",
    "\n",
    "        # collect parameters for gradient descent\n",
    "        self.params = [self.W, self.b]\n",
    "        # pull in parameters from the previously collected lists\n",
    "        for ae in self.hidden_layers:\n",
    "            self.params += ae.forward_params\n",
    "\n",
    "        # for momentum\n",
    "        self.dW = theano.shared(np.zeros(W0.shape), \"dW_logreg\")\n",
    "        self.db = theano.shared(np.zeros(K), \"db_logreg\")\n",
    "        self.dparams = [self.dW, self.db]\n",
    "        for ae in self.hidden_layers:\n",
    "            self.dparams += ae.forward_dparams\n",
    "\n",
    "        # define our inputs and our outputs\n",
    "        X_in = T.matrix('X_in')\n",
    "        targets = T.ivector('Targets')\n",
    "\n",
    "        # probability of Y given X\n",
    "        pY = self.forward(X_in)\n",
    "\n",
    "        # define the regularization cost:\n",
    "        # squared_magnitude = [(p*p).sum() for p in self.params]\n",
    "        # reg_cost = T.sum(squared_magnitude)\n",
    "        # previously we treated the targets as an indicator matrix and the output of a NN as a matrix of outputs\n",
    "        # we will only select the elements of pY that we want, which is where the targets are 1\n",
    "        cost = -T.mean( T.log(pY[T.arange(pY.shape[0]), targets]) ) #+ reg*reg_cost\n",
    "        # need prediction function to calculate the error rate\n",
    "        prediction = self.predict(X_in)\n",
    "        cost_predict_op = theano.function(\n",
    "            inputs=[X_in, targets],\n",
    "            outputs=[cost, prediction],\n",
    "        )\n",
    "\n",
    "        updates = [\n",
    "            (p, p + mu*dp - learning_rate*T.grad(cost, p)) for p, dp in zip(self.params, self.dparams)\n",
    "        ] + [\n",
    "            (dp, mu*dp - learning_rate*T.grad(cost, p)) for p, dp in zip(self.params, self.dparams)\n",
    "        ]\n",
    "        # updates = [(p, p - learning_rate*T.grad(cost, p)) for p in self.params]\n",
    "        train_op = theano.function(\n",
    "            inputs=[X_in, targets],\n",
    "            updates=updates,\n",
    "        )\n",
    "\n",
    "        n_batches = N / batch_sz\n",
    "        costs = []\n",
    "        print(\"supervised training...\")\n",
    "        for i in range(epochs):\n",
    "            print(\"epoch:\", i)\n",
    "            X, Y = shuffle(X, Y)\n",
    "            for j in range(int(n_batches)):\n",
    "                Xbatch = X[j*batch_sz:(j*batch_sz + batch_sz)]\n",
    "                Ybatch = Y[j*batch_sz:(j*batch_sz + batch_sz)]\n",
    "                train_op(Xbatch, Ybatch)\n",
    "                the_cost, the_prediction = cost_predict_op(Xtest, Ytest)\n",
    "                error = error_rate(the_prediction, Ytest)\n",
    "                if j == 408:\n",
    "                    print(\"j / n_batches:\", j, \"/\", n_batches, \"cost:\", the_cost, \"error:\", error)\n",
    "                costs.append(the_cost)\n",
    "        plt.plot(costs)\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, X):\n",
    "        return T.argmax(self.forward(X), axis=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        current_input = X\n",
    "        for ae in self.hidden_layers:\n",
    "            Z = ae.forward_hidden(current_input)\n",
    "            current_input = Z\n",
    "\n",
    "        # logistic layer\n",
    "        Y = T.nnet.softmax(T.dot(current_input, self.W) + self.b)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training autoencoder: 0\n",
      "epoch: 0\n",
      "j / n_batches: 408 / 410.0 cost: 50.61025039214906\n",
      "training autoencoder: 1\n",
      "epoch: 0\n",
      "j / n_batches: 408 / 410.0 cost: 102.46950189031583\n",
      "training autoencoder: 2\n",
      "epoch: 0\n",
      "j / n_batches: 408 / 410.0 cost: 57.48735243756561\n",
      "supervised training...\n",
      "epoch: 0\n",
      "j / n_batches: 408 / 410.0 cost: 0.21723407616486926 error: 0.06\n",
      "epoch: 1\n",
      "j / n_batches: 408 / 410.0 cost: 0.15267308577146935 error: 0.041\n",
      "epoch: 2\n",
      "j / n_batches: 408 / 410.0 cost: 0.1293009091252335 error: 0.035\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f279fad0fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xtrain, Ytrain, Xtest, Ytest = getKaggleMNIST()\n",
    "dnn = DNN([1000, 750, 500])\n",
    "dnn.fit(Xtrain, Ytrain, Xtest, Ytest, epochs=3)\n",
    "# vs\n",
    "#dnn = DNN([1000, 750, 500])\n",
    "#dnn.fit(Xtrain, Ytrain, Xtest, Ytest, pretrain=False, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
