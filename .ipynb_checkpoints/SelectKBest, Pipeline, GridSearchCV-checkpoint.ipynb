{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SelectKBest, Pipeline, GridSearchCV\n",
    "\n",
    "In this module, we will look at three different sklearn tools that help with feature selection, model building streamlining, and hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "Instead of going through the fitting and transformation steps for the training and test dataset separately, we can chain the StandardScaler, PCA, and LogisticRegression objects in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()\n",
    "X = pd.DataFrame(boston['data'], columns=boston['feature_names'])\n",
    "\n",
    "del X['CHAS']\n",
    "y = boston['target']\n",
    "y = np.array(y > y.mean()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pipeline object takes a list of tuples as input, where the first value in each tuple is an arbitrary identifier string that we can use to access the individual elements in the pipeline, as we will see later in this chapter, and the second element in every tuple is a scikit-learn transformer or estimator.\n",
    "\n",
    "The intermediate steps in a pipeline constitute scikit-learn transformers, and the last step is an estimator. In the preceding code example, we built a pipeline that consisted of two intermediate steps, a StandardScaler and a PCA transformer, and a logistic regression classifier as a final estimator. When we executed the fit method on the pipeline pipe_lr, the StandardScaler performed fit and transform on the training data, and the transformed training data was then passed onto the next object in the pipeline, the PCA. Similar to the previous step, PCA also executed fit and transform on the scaled input data and passed it to the final element of the pipeline, the estimator. We should note that there is no limit to the number of intermediate\n",
    "steps in this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.783\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([('scl', StandardScaler()),\n",
    "    ('pca', PCA(n_components=3)),\n",
    "    ('rf', RandomForestClassifier())])\n",
    "\n",
    "pipeline = fit(X_train, y_train)\n",
    "print('Test Accuracy: %.3f' % pipe_lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelectKBest\n",
    "\n",
    "The SelectKBest method is used in conjunction with the Pipeline() method. The Pipeline() method allows you to apply a series of transformations to some final estimator object. You can tell that something is a transformation because it will have a .tranform() method. SelectKBest() is a transformation. Another example is sklearn.preprocessing.StandardScaler(), which can be used to normalize your features. \n",
    "\n",
    "Note that the last object in the Pipeline argument should be the classifier / estimator you're going to use. \n",
    "\n",
    "### How does SelectKBest work?\n",
    "\n",
    "SelectKBest applies a univariate analysis, testing each available feature against a given label. The \"k\" features that score the best are noted. By applying the SelectKBest.transform() method to your classifier / estimator, you can reduce (simplify) it to only look at the 'K' best features in your data set. You can specify the scoring algorithm that SelectKBest will use. The most common options will be:\n",
    "\n",
    "+ __f_classif__ - ANOVA F-value between labe/feature for classification tasks.\n",
    "+ __f_regression__ - F-value between label/feature for regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.822\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Create SelectKBest() Object\n",
    "kbest = SelectKBest(k=5)\n",
    "\n",
    "# Create the Pipeline() obejct, and then fit it against the data set. \n",
    "pipeline = Pipeline([('scl', StandardScaler()),\n",
    "    ('kbest', SelectKBest(k=5)),\n",
    "    ('rf', RandomForestClassifier(random_state=1))])\n",
    "  \n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Let's see how our SelectKBest classifier performed\n",
    "print '\\n', 'Test Accuracy: %.3f' % pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectKBest Feature Selections:\n",
      "--------------------------------------------------\n",
      "CRIM >  False\n",
      "ZN >  False\n",
      "INDUS >  True\n",
      "NOX >  False\n",
      "RM >  True\n",
      "AGE >  True\n",
      "DIS >  False\n",
      "RAD >  False\n",
      "TAX >  False\n",
      "PTRATIO >  True\n",
      "B >  False\n",
      "LSTAT >  True\n"
     ]
    }
   ],
   "source": [
    "# this shows you which fields were selected\n",
    "print 'SelectKBest Feature Selections:'\n",
    "print 50 * '-'\n",
    "for f, p in zip(X.columns, pipeline.named_steps['kbest'].get_support()):\n",
    "    print f, '> ', p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectKBest Freature Scores:\n",
      "--------------------------------------------------\n",
      "CRIM >  28.4901903277\n",
      "ZN >  56.8925775276\n",
      "INDUS >  117.814262359\n",
      "NOX >  80.6977615043\n",
      "RM >  149.87988113\n",
      "AGE >  93.1370582776\n",
      "DIS >  33.2661738285\n",
      "RAD >  48.1654844612\n",
      "TAX >  82.3217957654\n",
      "PTRATIO >  118.054422134\n",
      "B >  23.034010459\n",
      "LSTAT >  269.476769308\n"
     ]
    }
   ],
   "source": [
    "# this shows you each fields' scores\n",
    "print 'SelectKBest Freature Scores:'\n",
    "print 50 * '-'\n",
    "for f, p in zip(X.columns, pipeline.named_steps['kbest'].scores_):\n",
    "    print f, '> ', p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectKBest Freature p-values:\n",
      "--------------------------------------------------\n",
      "CRIM >  1.69129059729e-07\n",
      "ZN >  3.95945854788e-13\n",
      "INDUS >  7.20495038008e-24\n",
      "NOX >  1.62493444336e-17\n",
      "RM >  5.94154805009e-29\n",
      "AGE >  1.04745354572e-19\n",
      "DIS >  1.76075304334e-08\n",
      "RAD >  1.89190751618e-11\n",
      "TAX >  8.33824390776e-18\n",
      "PTRATIO >  6.58028085655e-24\n",
      "B >  2.35622743573e-06\n",
      "LSTAT >  2.27483786418e-45\n"
     ]
    }
   ],
   "source": [
    "# this shows you each fields' p-values\n",
    "print 'SelectKBest Freature p-values:'\n",
    "print 50 * '-'\n",
    "for f, p in zip(X.columns, pipeline.named_steps['kbest'].pvalues_):\n",
    "    print f, '> ', p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV\n",
    "\n",
    "Unlike parameters that are learned from the data and created by the model, hyperparameters are parameters that are tuned prior to model creation, and thus need to be optimized separately. Grid search is a hyperparameter optimization technique used to optimize model performance by finding the optimal combination of hyperparameter values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SelectKBest/GridSearch - Accuracy:  0.828947368421\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "# Create a grid_search_dict. Using these with a pipeline can be a little tricky - note that\n",
    "#    you have to specify which pipeline component you want to grid search by using the alias\n",
    "#    you provided when you created the Pipeline object + '__' + [argument name]. \n",
    "grid_search_dict = {'kbest__k': range(1, len(X.columns)+1)}     \n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=grid_search_dict, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Let's see how our SelectKBest/GridSearch classifier performed\n",
    "print 'SelectKBest/GridSearch - Accuracy: ', accuracy_score(y_test, grid_search.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scl', StandardScaler(copy=True, with_mean=True, with_std=True)), ('kbest', SelectKBest(k=10, score_func=<function f_classif at 0x000000000BE123C8>)), ('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_...estimators=10, n_jobs=1, oob_score=False, random_state=1,\n",
       "            verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You lose the ability to see which fields were selected. \n",
    "# However, you can still see the final optimized output by calling the following attribute\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: do we need to redefine the model using the `best_estimator_` feature and then refit and recalculate the score? It seems that we get the same answer above..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectKBest/GridSearch - Accuracy:  0.828947368421\n"
     ]
    }
   ],
   "source": [
    "grid_search = grid_search.best_estimator_\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Let's see how our SelectKBest/GridSearch classifier performed\n",
    "print 'SelectKBest/GridSearch - Accuracy: ', grid_search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Raschka, Sebastian. _Python Machine Learning_. Packt Publishing, 2015, Birmingham, UK.\n",
    "- sklearn - Univariate Feature Selection: http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\n",
    "- sklearn.feature_selection.SelectKBest: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\n",
    "- sklearn.pipeline.Pipeline: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "- sklearn.grid_search.GridSearchCV: http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html\n",
    "- Alternative example on everything covered here: https://civisanalytics.com/blog/data-science/2016/01/06/workflows-python-using-pipeline-gridsearchcv-for-compact-code/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
