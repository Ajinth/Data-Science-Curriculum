{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation Techniques\n",
    "\n",
    "A proper model and its optimal set of features and parameter specifications is a complex process that defines the success of an algorithm. We estimate this success using our chosen model metrics. However, in order for the metrics to be accurate, we need need to have confidence that the performance of the model will generalize well to unseen data. This is where cross-validation comes in; using these techniques, we divide our data into training and test datasets in order to get a good estimate of the model's generalization error as well as choose the appropriate bias-variance trade-off.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The holdout method  \n",
    "\n",
    "In the holdout method, we partition the data into a training and test set, where the former is used for training the model and the latter is used for evaluating the model. However, it is very likely that we will have many different versions of a given model which we will want to evaluate, with each evaluation informing the decisions made in the subsequent version of the model. If we use (and reuse) the test set when tuning our model, then the test data is being used for training, which means that we are likely to overfit our model to not only the training set, but to the test set as well, rendering our model metrics inaccurate. \n",
    "\n",
    "In order to prevent that we want to further subset our training set into a training and validation set. Now we can use the validation set for the model selection and use the test set only once we have chosen our model and would like to get an accurate measure of the model's generalization error. The advantage of having a test set that the model hasn't seen before during the training and model selection steps is that we can obtain a less biased estimate of its ability to generalize to new data.\n",
    "\n",
    "<img src=\"extras/06_02.png\" width=\"500\" height=\"500\" />\n",
    "\n",
    "A disadvantage of the holdout method is that the performance estimate is sensitive to how we partition the training set into the training and validation subsets; the estimate will vary for different samples of the data. Additionally, the validation set might contain some valuable information that could be useful in developing our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross-validation \n",
    "\n",
    "In k-fold cross-validation, we randomly split the training dataset into k folds without replacement, where k-1 folds are used for the model training and one fold is used for testing. This procedure is repeated k times so that we obtain k models and performance estimates. \n",
    "\n",
    "We then calculate the average performance of the models based on the different, independent folds to obtain a performance estimate that is less sensitive to the subpartitioning of the training data compared to the holdout method. Typically, we use k-fold cross-validation for model tuning, that is, finding the optimal hyperparameter values that yield a satisfying generalization performance. Once we have found satisfactory hyperparameter values, we can retrain the model on the complete training set and obtain a final performance estimate using the independent test set.\n",
    "\n",
    "Since k-fold cross-validation is a resampling technique without replacement, the advantage of this approach is that each sample point will be part of a training and test dataset exactly once, which yields a lower-variance estimate of the model performance than the holdout method. The following figure summarizes the concept behind k-fold cross-validation with k=10. The training data set is divided into 10 folds, and during the 10 iterations, 9 folds are used for training, and 1 fold will be used as the test set for the model evaluation. Also, the estimated performances Ei (for example, classification accuracy or error) for each fold are then used to calculate the estimated average performance E of the model:\n",
    "\n",
    "<img src=\"extras/06_03.png\" width=\"600\" height=\"600\" />\n",
    "\n",
    "The value of k varies depending on the size of our dataset. We generally set k to 10, but for smaller datasets we can increase it, or for larger sets we can decrease it. A higher k ensures that we use more of our training data in estimating the model, which will result in a lower bias; however this will also cause a higher variance due to overfitting that is caused by using the same training samples multiple times. Additionally, the higher the k, the higher the computational cost of fitting and evaluating our model, which is why we want to use a lower k for large datasets. \n",
    "\n",
    "The extreme case of k is known as the _leave one out method_, where the number of folds equals the number of samples in the training set (k = n), so that only one sample is used for testing during each iteration. This approach is preferred when our dataset is extrememly small.\n",
    "\n",
    "In _stratified k-fold cross-validation_, the class proportions are preserved in each fold in order to ensure that each fold is representative of the class proportions in the training set. In the case of unequal class proportions, this method has shown to deliver better bias and variance estimates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested k-fold cross-validation \n",
    "\n",
    "If we are choosing the optimal set of features and parameter specifications of a given algorithm, then we might want to use k-fold cross-validation. However, if additionally we want to choose between multiple algorithms, then we will need to implement the nested k-fold cross-validation method. This method allows us to get an even more accurate estimate of the model performance so that we can choose between multiple algorithms. \n",
    "\n",
    "In nested cross-validation, we have an outer k-fold cross-validation loop to split the data into training and test folds, and an inner loop is used to select the model using k-fold cross-validation on the training fold. After model selection, the test fold is then used to evaluate the model performance. The following figure explains the concept of nested cross-validation with five outer and two inner folds, which can be useful for large data sets where computational performance is important; this particular type of nested cross-validation is also known as 5x2 cross-validation:\n",
    "\n",
    "<img src=\"extras/06_07.png\" width=\"500\" height=\"500\" />\n",
    "\n",
    "Nested cross-validation is used to avoid optimistically biased estimates of performance that result from using the same cross-validation to set the values of the hyper-parameters of the model (e.g. the regularisation parameter, C, and kernel parameters of an SVM) and performance estimation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important caveats\n",
    "\n",
    "When doing k-fold cross-validation, we must be careful not to select our features before doing the train-test splits. Instead, feature selection needs to be part of the process within each of the k iterations. This will prevent _knowledge leaking_ from the test set to the training set, which would otherwise result in a biased estimate of the model evaluation metric (it would be overly optimistic). If there the different iterations recommend different features, then we can use a voting method to pick the top performing features and use those to fit the final model.  \n",
    "\n",
    "In the case of imbalanced data (in the class of the target), we might want to consider oversampling the minority class or undersampling the majority class during cross-validation (but after the fold split). This will result in more accurate measures of the model metrics (sensitivity, specificity, AUC). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Example\n",
    "\n",
    "In our example, we will use the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/titanic_train.csv')\n",
    "\n",
    "X = pd.concat([pd.get_dummies(data['Sex'], drop_first=True),data['Fare']], axis=1)\n",
    "y = data['Survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-fold cross-validation\n",
    "\n",
    "We can use sklearn's `cross_val_score` to perform our k-fold cross-validation and return the individual and average scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy scores: [ 0.71428571  0.74603175  0.84126984  0.76190476  0.87301587  0.79365079\n",
      "  0.79032258  0.81967213  0.83606557  0.75409836]\n",
      "CV accuracy: 0.793 +/- 0.047\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C=1000, random_state=0)\n",
    "\n",
    "scores = cross_val_score(estimator=lr,\n",
    "                         X=X_train,\n",
    "                         y=y_train,\n",
    "                         cv=10,\n",
    "                         n_jobs=1)\n",
    "print('CV accuracy scores: %s' % scores)\n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can implement k-fold cross-validation when finding the optimal hyperparameters in using GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.792937399679\n",
      "{'clf__C': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),\n",
    "            ('clf', lr)])\n",
    "\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "\n",
    "param_grid = [{'clf__C': param_range, \n",
    "               }]\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe_lr, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='accuracy', \n",
    "                  cv=10,\n",
    "                  n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nested k-fold cross-validation\n",
    "\n",
    "Let's use nested k-fold cross-validation to perform compare the logistic regression model to a support vector machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy: 0.793 +/- 0.035\n"
     ]
    }
   ],
   "source": [
    "gs = GridSearchCV(estimator=pipe_lr,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv=2)\n",
    "\n",
    "# Note: Optionally, you could use cv=2 \n",
    "# in the GridSearchCV above to produce\n",
    "# the 5 x 2 nested CV that is shown in the figure.\n",
    "\n",
    "scores = cross_val_score(gs, X_train, y_train, scoring='accuracy', cv=5)\n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy: 0.769 +/- 0.058\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "param_range = [0.0001, 1000.0]\n",
    "\n",
    "param_grid = [{'C': param_range, \n",
    "               'kernel': ['rbf']}]\n",
    "\n",
    "gs = GridSearchCV(estimator=SVC(),\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv=2,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "scores = cross_val_score(gs, X_train, y_train, scoring='accuracy', cv=5)\n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- http://www.marcoaltini.com/blog/dealing-with-imbalanced-data-undersampling-oversampling-and-proper-cross-validation\n",
    "- http://www.alfredo.motta.name/cross-validation-done-wrong/\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "- Raschka, Sebastian. Python Machine Learning. Packt Publishing, 2015, Birmingham, UK.\n",
    "- http://stats.stackexchange.com/questions/27750/feature-selection-and-cross-validation"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
