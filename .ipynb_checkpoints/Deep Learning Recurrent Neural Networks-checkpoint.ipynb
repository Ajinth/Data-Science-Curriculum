{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The Simple Recurrent Unit  \n",
    "\n",
    "Recurrent neural networkds (RNNs) are a type of artificial neural network designed to recognize patterns in sequences of data, such as text, genomes, handwriting, the spoken word, or numerical times series data emanating from sensors, stock markets and government agencies. The simple recurrent unit, also known as the Elman unit, is the most basic form of RNNs.  The way an RNN works is that it introduces a feedback look from the output of the hidden layer back into itself, where it uses uses the previous output of the hidden layer as one of its inputs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![](extras/rnn.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here $h(t)$ is an M-sized vector, and the weight matrix of the hidden layer $W_h$ is $MXM$. There are m-hidden units, and each one connects back to all the previous hidden units, meaning in total there will be $M^2$ hidden weights.  \n",
    "\n",
    "With recurrent units, our data has a new dimensionality, which is the length of a sequence, T.  Often times, the sequences are not of equal length, so we have to store them in a list.  \n",
    "\n",
    "Here is how we represent the hidden layer and the output layer of the RNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\\begin{align}\n",
    "\\ h(t) = f(W_h^Th(t-1)+W_x^Tx(t)+b_h\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\\begin{align}\n",
    "\\ y(t) = softmax(W_o^Th(t)+b_o)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "where f = sigmoid, tanh, relu. Note that the hidden layer equation can be upacked further: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\\begin{align}\n",
    "\\ h(t) = f(W_h^Th(t-1)+W_x^Tx(t)+b_h\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\\begin{align}\n",
    "\\ h(t) = f(W_h^T(f(W_h^Th(t-2)+W_x^Tx(t-1)+b_h)+W_x^Tx(t)+b_h)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "etc., where $h(0)$ is a hyperparameter that can be set to zero or be made update-able."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Additionally, we can add more hidden layers, each with it's own recurrence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![](extras/rnn_layered.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There are three types of predicitons we can make using RNNs:\n",
    "1. Predict one label over an entire sequence (e.g. differentiate between male and female voices).\n",
    "2. Predict a label for every step of input sequence (e.g. control device using BCI).\n",
    "3. Predict next value in a sequence (e.g. next word in a sentence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Another key concept here is _shared weights_.  \n",
    "RNNs rely on 'backpropagation through time,' which is just a fancy way of saying backpropagation, which is just the same as gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Vanishing gradient and exploding gradient can be issues with RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Rated Recurrent Unit   \n",
    "\n",
    "The rated recurrent unit is a simple modification to the basic RNN. \n",
    "\n",
    "e want to weight two things:  \n",
    "1. $f(x(t), h(t-1))$ -> output we would have gotten from a regular recurrent unit.\n",
    "2. $h(t-1)$ -> previous value of hidden state.\n",
    "\n",
    "We use a matrix state called the rate matrix, $z$, which is the same size as the hidden layer. We then do an element-by-element multiplication on each dimension:  \n",
    "\n",
    "$h(t) = 1-z \\circ h(t-1) + z \\circ f(x(t), h(t-1))$\n",
    "\n",
    "The gate we add is supposed to be a lot like a low-pass filter. $z$ can be calculated multiple ways:\n",
    "- it can be a full-size matrix (MXM)\n",
    "- Can also make $z(t) = f(x(t), h(t-1)) = f(W_{xz}x(t)+W_{hz}h(t-1)+b_z)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![](extras/rated_recurrence.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Gated Recurrent Unit  \n",
    "\n",
    "Established in 2014, the gated recurrent unit (GRU), has comparable performance to LSTM, though it uses fewer parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![](extras/gru.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\\begin{array}{lcl}\n",
    "r_t = \\sigma(x_tW_{xr}+h_{t-1}W_{hr}+b_r) \\\\\n",
    "z_t = \\sigma(x_tW_{xz}+h_{t-1}W_{hz}+b_z) \\\\\n",
    "\\hat{h}_t=g(x_tW_{xh}+(r_t \\odot h_{t-1})W_{hh} + b_h) \\\\\n",
    "h_t = (1-z_t) \\odot h_{t-1} + z_t \\odot \\hat{h}_t \\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "where $g()$ is the activation function and the _circle-dot_ symbol signifies element-wise multiplication.  \n",
    "\n",
    "If $r(t) = 0$, we get $\\hat{h}_t = g(x_t)W_{xh}+b_h)$, which is like the beginning of a new sequence.  \n",
    "\n",
    "Note that this is note the full picture, since it doesn't consider h(0), and $\\hat{h}_t$ is only a candidate for the new h(t). Thus, h(t) will be a combo of h(t-1) and $\\hat{h}_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Long Short-Term Memory (LSTM)   \n",
    "\n",
    "LSTMs are composed of:\n",
    "- 3 gates: input, output, and forget gate\n",
    "- a memory cell $c_t$ (no more $\\hat{h}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![](extras/lstm.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\\begin{array}{lcl}\n",
    "i_t = \\sigma(x_tW_{xi}+h_{t-1}W_{hi}+c_{t-1}W_{ci}+b_i) \\\\\n",
    "f_t = \\sigma(x_tW_{xf}+h_{t-1}W_{hf}+c_{t-1}W_{cf}+b_f) \\\\\n",
    "c_t=f_tc_{t-1}+i_ttanh(x_tW_{xc}+h_{t-1}W_{hc} + b_c) \\\\\n",
    "o_t = \\sigma(x_tW_{xo}+h_{t-1}W_{ho}+c_tW_{co} + b_o \\\\\n",
    "h_t = o_ttanh(c_t)\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "LSTM Parameters:   \n",
    "Input gate $_t$ determines how much of the new value goes into the cell.\n",
    "- Parameters: $W_{xi}, W_{hi}, W_{ci}, b_i$\n",
    "- Depends on: $x_t, h_{t-1}, c_{t-1}$  \n",
    "\n",
    "Forget gate $f_t$ determines how much of the previous cell value goes into the current cell value.  \n",
    "- Parameters: $W_{xf}, W_{hf}, W_{cf}, b_f$\n",
    "- Depends on: $x_t, h_{t-1}, c_{t-1}$  \n",
    "\n",
    "Candidate cell $c_t$ looks like the simple recurrent unit right before it gets multiplied by the input gate.  \n",
    "- Parameters: $W_{xc}, W_{hc}, b_c$\n",
    "- Depends on: $x_t, h_{t-1}$  \n",
    "\n",
    "Output gate $o_t$ takes into account everything (input at time t, the previous hidden state, and the current cell value). \n",
    "- Parameters: $W_{xo}, W_{ho}, W_{co}, b_o$\n",
    "- Depends on: $x_t, h_{t-1}, c_t$\n",
    "\n",
    "New hidden state $h_t$ is the tanh of the cell value times the output gate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Coded Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from util import init_weight, all_parity_pairs_with_sequence_labels\n",
    "\n",
    "\n",
    "class SimpleRNN:\n",
    "    def __init__(self, M):\n",
    "        self.M = M # hidden layer size\n",
    "\n",
    "    def fit(self, X, Y, learning_rate=10e-1, mu=0.99, reg=1.0, activation=T.tanh, epochs=100, show_fig=False):\n",
    "        D = X[0].shape[1] # X is of size N x T(n) x D\n",
    "        K = len(set(Y.flatten()))\n",
    "        N = len(Y)\n",
    "        M = self.M\n",
    "        self.f = activation\n",
    "\n",
    "        # initial weights\n",
    "        Wx = init_weight(D, M)\n",
    "        Wh = init_weight(M, M)\n",
    "        bh = np.zeros(M)\n",
    "        h0 = np.zeros(M)\n",
    "        Wo = init_weight(M, K)\n",
    "        bo = np.zeros(K)\n",
    "\n",
    "        # make them theano shared\n",
    "        self.Wx = theano.shared(Wx)\n",
    "        self.Wh = theano.shared(Wh)\n",
    "        self.bh = theano.shared(bh)\n",
    "        self.h0 = theano.shared(h0)\n",
    "        self.Wo = theano.shared(Wo)\n",
    "        self.bo = theano.shared(bo)\n",
    "        self.params = [self.Wx, self.Wh, self.bh, self.h0, self.Wo, self.bo]\n",
    "\n",
    "        thX = T.fmatrix('X')\n",
    "        thY = T.ivector('Y')\n",
    "\n",
    "        def recurrence(x_t, h_t1):\n",
    "            # returns h(t), y(t)\n",
    "            h_t = self.f(x_t.dot(self.Wx) + h_t1.dot(self.Wh) + self.bh)\n",
    "            y_t = T.nnet.softmax(h_t.dot(self.Wo) + self.bo)\n",
    "            return h_t, y_t\n",
    "\n",
    "        [h, y], _ = theano.scan(\n",
    "            fn=recurrence,\n",
    "            outputs_info=[self.h0, None],\n",
    "            sequences=thX,\n",
    "            n_steps=thX.shape[0],\n",
    "        )\n",
    "\n",
    "        py_x = y[:, 0, :]\n",
    "        prediction = T.argmax(py_x, axis=1)\n",
    "\n",
    "        cost = -T.mean(T.log(py_x[T.arange(thY.shape[0]), thY]))\n",
    "        grads = T.grad(cost, self.params)\n",
    "        dparams = [theano.shared(p.get_value()*0) for p in self.params]\n",
    "\n",
    "        updates = [\n",
    "            (p, p + mu*dp - learning_rate*g) for p, dp, g in zip(self.params, dparams, grads)\n",
    "        ] + [\n",
    "            (dp, mu*dp - learning_rate*g) for dp, g in zip(dparams, grads)\n",
    "        ]\n",
    "\n",
    "        self.predict_op = theano.function(inputs=[thX], outputs=prediction)\n",
    "        self.train_op = theano.function(\n",
    "            inputs=[thX, thY],\n",
    "            outputs=[cost, prediction, y],\n",
    "            updates=updates\n",
    "        )\n",
    "\n",
    "        costs = []\n",
    "        for i in xrange(epochs):\n",
    "            X, Y = shuffle(X, Y)\n",
    "            n_correct = 0\n",
    "            cost = 0\n",
    "            for j in xrange(N):\n",
    "                c, p, rout = self.train_op(X[j], Y[j])\n",
    "                # print \"p:\", p\n",
    "                cost += c\n",
    "                if p[-1] == Y[j,-1]:\n",
    "                    n_correct += 1\n",
    "            print \"shape y:\", rout.shape\n",
    "            print \"i:\", i, \"cost:\", cost, \"classification rate:\", (float(n_correct)/N)\n",
    "            costs.append(cost)\n",
    "            if n_correct == N:\n",
    "                break\n",
    "\n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def parity(B=12, learning_rate=10e-5, epochs=200):\n",
    "    X, Y = all_parity_pairs_with_sequence_labels(B)\n",
    "\n",
    "    rnn = SimpleRNN(4)\n",
    "    rnn.fit(X, Y, learning_rate=learning_rate, epochs=epochs, activation=T.nnet.sigmoid, show_fig=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://deeplearningcourses.com/c/deep-learning-recurrent-neural-networks-in-python\n",
    "# https://udemy.com/deep-learning-recurrent-neural-networks-in-python\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from util import init_weight\n",
    "\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, Mi, Mo, activation):\n",
    "        self.Mi = Mi\n",
    "        self.Mo = Mo\n",
    "        self.f  = activation\n",
    "\n",
    "        # numpy init\n",
    "        Wxr = init_weight(Mi, Mo)\n",
    "        Whr = init_weight(Mo, Mo)\n",
    "        br  = np.zeros(Mo)\n",
    "        Wxz = init_weight(Mi, Mo)\n",
    "        Whz = init_weight(Mo, Mo)\n",
    "        bz  = np.zeros(Mo)\n",
    "        Wxh = init_weight(Mi, Mo)\n",
    "        Whh = init_weight(Mo, Mo)\n",
    "        bh  = np.zeros(Mo)\n",
    "        h0  = np.zeros(Mo)\n",
    "\n",
    "        # theano vars\n",
    "        self.Wxr = theano.shared(Wxr)\n",
    "        self.Whr = theano.shared(Whr)\n",
    "        self.br  = theano.shared(br)\n",
    "        self.Wxz = theano.shared(Wxz)\n",
    "        self.Whz = theano.shared(Whz)\n",
    "        self.bz  = theano.shared(bz)\n",
    "        self.Wxh = theano.shared(Wxh)\n",
    "        self.Whh = theano.shared(Whh)\n",
    "        self.bh  = theano.shared(bh)\n",
    "        self.h0  = theano.shared(h0)\n",
    "        self.params = [self.Wxr, self.Whr, self.br, self.Wxz, self.Whz, self.bz, self.Wxh, self.Whh, self.bh, self.h0]\n",
    "\n",
    "    def recurrence(self, x_t, h_t1):\n",
    "        r = T.nnet.sigmoid(x_t.dot(self.Wxr) + h_t1.dot(self.Whr) + self.br)\n",
    "        z = T.nnet.sigmoid(x_t.dot(self.Wxz) + h_t1.dot(self.Whz) + self.bz)\n",
    "        hhat = self.f(x_t.dot(self.Wxh) + (r * h_t1).dot(self.Whh) + self.bh)\n",
    "        h = (1 - z) * h_t1 + z * hhat\n",
    "        return h\n",
    "\n",
    "    def output(self, x):\n",
    "        # input X should be a matrix (2-D)\n",
    "        # rows index time\n",
    "        h, _ = theano.scan(\n",
    "            fn=self.recurrence,\n",
    "            sequences=x,\n",
    "            outputs_info=[self.h0],\n",
    "            n_steps=x.shape[0],\n",
    "        )\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://deeplearningcourses.com/c/deep-learning-recurrent-neural-networks-in-python\n",
    "# https://udemy.com/deep-learning-recurrent-neural-networks-in-python\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from util import init_weight\n",
    "\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, Mi, Mo, activation):\n",
    "        self.Mi = Mi\n",
    "        self.Mo = Mo\n",
    "        self.f  = activation\n",
    "\n",
    "        # numpy init\n",
    "        Wxi = init_weight(Mi, Mo)\n",
    "        Whi = init_weight(Mo, Mo)\n",
    "        Wci = init_weight(Mo, Mo)\n",
    "        bi  = np.zeros(Mo)\n",
    "        Wxf = init_weight(Mi, Mo)\n",
    "        Whf = init_weight(Mo, Mo)\n",
    "        Wcf = init_weight(Mo, Mo)\n",
    "        bf  = np.zeros(Mo)\n",
    "        Wxc = init_weight(Mi, Mo)\n",
    "        Whc = init_weight(Mo, Mo)\n",
    "        bc  = np.zeros(Mo)\n",
    "        Wxo = init_weight(Mi, Mo)\n",
    "        Who = init_weight(Mo, Mo)\n",
    "        Wco = init_weight(Mo, Mo)\n",
    "        bo  = np.zeros(Mo)\n",
    "        c0  = np.zeros(Mo)\n",
    "        h0  = np.zeros(Mo)\n",
    "\n",
    "        # theano vars\n",
    "        self.Wxi = theano.shared(Wxi)\n",
    "        self.Whi = theano.shared(Whi)\n",
    "        self.Wci = theano.shared(Wci)\n",
    "        self.bi  = theano.shared(bi)\n",
    "        self.Wxf = theano.shared(Wxf)\n",
    "        self.Whf = theano.shared(Whf)\n",
    "        self.Wcf = theano.shared(Wcf)\n",
    "        self.bf  = theano.shared(bf)\n",
    "        self.Wxc = theano.shared(Wxc)\n",
    "        self.Whc = theano.shared(Whc)\n",
    "        self.bc  = theano.shared(bc)\n",
    "        self.Wxo = theano.shared(Wxo)\n",
    "        self.Who = theano.shared(Who)\n",
    "        self.Wco = theano.shared(Wco)\n",
    "        self.bo  = theano.shared(bo)\n",
    "        self.c0  = theano.shared(c0)\n",
    "        self.h0  = theano.shared(h0)\n",
    "        self.params = [\n",
    "            self.Wxi,\n",
    "            self.Whi,\n",
    "            self.Wci,\n",
    "            self.bi,\n",
    "            self.Wxf,\n",
    "            self.Whf,\n",
    "            self.Wcf,\n",
    "            self.bf,\n",
    "            self.Wxc,\n",
    "            self.Whc,\n",
    "            self.bc,\n",
    "            self.Wxo,\n",
    "            self.Who,\n",
    "            self.Wco,\n",
    "            self.bo,\n",
    "            self.c0,\n",
    "            self.h0,\n",
    "        ]\n",
    "\n",
    "    def recurrence(self, x_t, h_t1, c_t1):\n",
    "        i_t = T.nnet.sigmoid(x_t.dot(self.Wxi) + h_t1.dot(self.Whi) + c_t1.dot(self.Wci) + self.bi)\n",
    "        f_t = T.nnet.sigmoid(x_t.dot(self.Wxf) + h_t1.dot(self.Whf) + c_t1.dot(self.Wcf) + self.bf)\n",
    "        c_t = f_t * c_t1 + i_t * T.tanh(x_t.dot(self.Wxc) + h_t1.dot(self.Whc) + self.bc)\n",
    "        o_t = T.nnet.sigmoid(x_t.dot(self.Wxo) + h_t1.dot(self.Who) + c_t.dot(self.Wco) + self.bo)\n",
    "        h_t = o_t * T.tanh(c_t)\n",
    "        return h_t, c_t\n",
    "\n",
    "    def output(self, x):\n",
    "        # input X should be a matrix (2-D)\n",
    "        # rows index time\n",
    "        [h, c], _ = theano.scan(\n",
    "            fn=self.recurrence,\n",
    "            sequences=x,\n",
    "            outputs_info=[self.h0, self.c0],\n",
    "            n_steps=x.shape[0],\n",
    "        )\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References  \n",
    "\n",
    "- https://deeplearning4j.org/lstm\n",
    "- https://www.udemy.com/deep-learning-recurrent-neural-networks-in-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
