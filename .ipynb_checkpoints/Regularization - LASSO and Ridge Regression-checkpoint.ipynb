{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization - LASSO and Ridge Regression\n",
    "\n",
    "Regularization is a method of penalizing extreme parameter weights in order to reduce overfitting, filter out noise from the data, and mitigate collinearity. The technique is used in regression modeling, whereby it introduces a penalty function that is is optimized in addition to the standard residual sum of squares. \n",
    "\n",
    "It is important to note that independent variables need to be scaled before performing regularization, or else the process will not function properly. \n",
    "\n",
    "The two primary methods of regularization are L1 and L2, where L1 regularization is used in LASSO regression and L2 regularization is used in Ridge Regression. \n",
    "\n",
    "\n",
    "\n",
    "## L2 Regularization with Ridge Regression\n",
    "\n",
    "L2 Regularization adds a factor of sum of squares of coefficients in the optimization objective. Thus, ridge regression optimizes the following:\n",
    "\n",
    "Objective (Cost) Function = RSS + λ * (sum of square of coefficients)\n",
    "\n",
    "<img src=\"extras/Ridge1.png\" width=\"700\" height=\"700\" />\n",
    "\n",
    "Here, λ (lambda) is the parameter which balances the amount of emphasis given to minimizing RSS vs minimizing sum of square of coefficients. λ can take various values:\n",
    "\n",
    "λ = 0:  \n",
    "- The objective becomes same as simple linear regression.\n",
    "- We’ll get the same coefficients as simple linear regression.  \n",
    "\n",
    "λ = ∞:  \n",
    "- The coefficients will be zero. Why? Because of infinite weightage on square of coefficients, anything less than zero will make the objective infinite.   \n",
    "\n",
    "0 < λ < ∞:\n",
    "- The magnitude of α will decide the weightage given to different parts of objective.\n",
    "- The coefficients will be somewhere between 0 and ones for simple linear regression.\n",
    "\n",
    "To better understand this how lambda affects the coefficients, et's follow the process of the regression algorithm. It tries to minimze the cost function above by performing a gradient descent. In order to determine the gradient for given weights, it must perform a partial derivative with respect to a particular weight (wj):\n",
    "\n",
    "<img src=\"extras/Ridge2.png\" width=\"700\" height=\"700\" />\n",
    "\n",
    "We then update the jth weight by subtracting the learning rate times the gradient, resulting in:\n",
    "\n",
    "<img src=\"extras/Ridge3.png\" width=\"700\" height=\"700\" />\n",
    "\n",
    "The first term on the right-hand side of the equation tells us that ridge regression is equivalent to reducing the weight by a factor of (1-2λη) first and then applying the same update rule as simple linear regression. Here we can see how different sizes of lambda can affect the weights of the coeffiecients, and how the coefficients can never fully be zero. \n",
    "\n",
    "## Ridge Regression Coding Example\n",
    "\n",
    "We are going to use the Boston housing dataset to perform our examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(boston['data'], columns=boston['feature_names'])\n",
    "y = pd.Series(boston['target'], name='MED')\n",
    "bos = pd.concat([y, X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "alpha_ridge = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n",
    "\n",
    "coeffs = {}\n",
    "z = 1.01\n",
    "for alpha in alpha_ridge:\n",
    "    r = Ridge(alpha=alpha, normalize=True)\n",
    "    r = r.fit(X, y)\n",
    "    coeffs[str(z)+':'+str(alpha)] = np.append(r.score(X,y), np.append(r.intercept_, r.coef_))\n",
    "    z += .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the coefficients for the different values of alpha, what we see is that their magnitude for every predictor (except for DIS at alpha=5 and RAD at alpha=1). As the impact of each predictor decreases, the model increases in simplicity, meaning underfitting begins to take place. We confirm this by looking at the model score, which begins to drop drastically after alpha reaches 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SCORE</th>\n",
       "      <th>INTERCEPT</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.01:1e-15</th>\n",
       "      <td>0.740608</td>\n",
       "      <td>36.491103</td>\n",
       "      <td>-0.107171</td>\n",
       "      <td>0.046395</td>\n",
       "      <td>0.020860</td>\n",
       "      <td>2.688561</td>\n",
       "      <td>-17.795759</td>\n",
       "      <td>3.804752</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>-1.475759</td>\n",
       "      <td>0.305655</td>\n",
       "      <td>-0.012329</td>\n",
       "      <td>-0.953464</td>\n",
       "      <td>0.009393</td>\n",
       "      <td>-0.525467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.02:1e-10</th>\n",
       "      <td>0.740608</td>\n",
       "      <td>36.491103</td>\n",
       "      <td>-0.107171</td>\n",
       "      <td>0.046395</td>\n",
       "      <td>0.020860</td>\n",
       "      <td>2.688561</td>\n",
       "      <td>-17.795759</td>\n",
       "      <td>3.804752</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>-1.475759</td>\n",
       "      <td>0.305655</td>\n",
       "      <td>-0.012329</td>\n",
       "      <td>-0.953464</td>\n",
       "      <td>0.009393</td>\n",
       "      <td>-0.525467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.03:1e-08</th>\n",
       "      <td>0.740608</td>\n",
       "      <td>36.491101</td>\n",
       "      <td>-0.107171</td>\n",
       "      <td>0.046395</td>\n",
       "      <td>0.020860</td>\n",
       "      <td>2.688561</td>\n",
       "      <td>-17.795757</td>\n",
       "      <td>3.804753</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>-1.475759</td>\n",
       "      <td>0.305655</td>\n",
       "      <td>-0.012329</td>\n",
       "      <td>-0.953464</td>\n",
       "      <td>0.009393</td>\n",
       "      <td>-0.525467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.04:0.0001</th>\n",
       "      <td>0.740608</td>\n",
       "      <td>36.471735</td>\n",
       "      <td>-0.107121</td>\n",
       "      <td>0.046362</td>\n",
       "      <td>0.020681</td>\n",
       "      <td>2.689258</td>\n",
       "      <td>-17.783439</td>\n",
       "      <td>3.805387</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>-1.475111</td>\n",
       "      <td>0.305233</td>\n",
       "      <td>-0.012309</td>\n",
       "      <td>-0.953268</td>\n",
       "      <td>0.009392</td>\n",
       "      <td>-0.525373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.05:0.001</th>\n",
       "      <td>0.740605</td>\n",
       "      <td>36.299120</td>\n",
       "      <td>-0.106679</td>\n",
       "      <td>0.046064</td>\n",
       "      <td>0.019094</td>\n",
       "      <td>2.695421</td>\n",
       "      <td>-17.673424</td>\n",
       "      <td>3.811020</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>-1.469303</td>\n",
       "      <td>0.301487</td>\n",
       "      <td>-0.012127</td>\n",
       "      <td>-0.951519</td>\n",
       "      <td>0.009390</td>\n",
       "      <td>-0.524538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.06:0.01</th>\n",
       "      <td>0.740344</td>\n",
       "      <td>34.724275</td>\n",
       "      <td>-0.102710</td>\n",
       "      <td>0.043381</td>\n",
       "      <td>0.005483</td>\n",
       "      <td>2.748085</td>\n",
       "      <td>-16.652015</td>\n",
       "      <td>3.860450</td>\n",
       "      <td>-0.000289</td>\n",
       "      <td>-1.413705</td>\n",
       "      <td>0.268783</td>\n",
       "      <td>-0.010573</td>\n",
       "      <td>-0.935255</td>\n",
       "      <td>0.009365</td>\n",
       "      <td>-0.516574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.07:1</th>\n",
       "      <td>0.635014</td>\n",
       "      <td>21.026665</td>\n",
       "      <td>-0.059368</td>\n",
       "      <td>0.017702</td>\n",
       "      <td>-0.072405</td>\n",
       "      <td>2.311548</td>\n",
       "      <td>-3.927643</td>\n",
       "      <td>2.874591</td>\n",
       "      <td>-0.009297</td>\n",
       "      <td>-0.249665</td>\n",
       "      <td>-0.004520</td>\n",
       "      <td>-0.002736</td>\n",
       "      <td>-0.535678</td>\n",
       "      <td>0.006218</td>\n",
       "      <td>-0.261501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.08:5</th>\n",
       "      <td>0.415215</td>\n",
       "      <td>23.554065</td>\n",
       "      <td>-0.037414</td>\n",
       "      <td>0.012091</td>\n",
       "      <td>-0.054343</td>\n",
       "      <td>0.964243</td>\n",
       "      <td>-2.661259</td>\n",
       "      <td>1.193735</td>\n",
       "      <td>-0.009043</td>\n",
       "      <td>0.018627</td>\n",
       "      <td>-0.026814</td>\n",
       "      <td>-0.002104</td>\n",
       "      <td>-0.249294</td>\n",
       "      <td>0.003280</td>\n",
       "      <td>-0.111930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.09:10</th>\n",
       "      <td>0.297932</td>\n",
       "      <td>23.655470</td>\n",
       "      <td>-0.025734</td>\n",
       "      <td>0.008545</td>\n",
       "      <td>-0.038718</td>\n",
       "      <td>0.550144</td>\n",
       "      <td>-1.943350</td>\n",
       "      <td>0.708549</td>\n",
       "      <td>-0.006844</td>\n",
       "      <td>0.037962</td>\n",
       "      <td>-0.021554</td>\n",
       "      <td>-0.001513</td>\n",
       "      <td>-0.155247</td>\n",
       "      <td>0.002176</td>\n",
       "      <td>-0.068882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.1:20</th>\n",
       "      <td>0.191601</td>\n",
       "      <td>23.380079</td>\n",
       "      <td>-0.015848</td>\n",
       "      <td>0.005351</td>\n",
       "      <td>-0.024335</td>\n",
       "      <td>0.295135</td>\n",
       "      <td>-1.243978</td>\n",
       "      <td>0.395494</td>\n",
       "      <td>-0.004452</td>\n",
       "      <td>0.032190</td>\n",
       "      <td>-0.014334</td>\n",
       "      <td>-0.000955</td>\n",
       "      <td>-0.089635</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>-0.039586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                SCORE  INTERCEPT      CRIM        ZN     INDUS      CHAS  \\\n",
       "1.01:1e-15   0.740608  36.491103 -0.107171  0.046395  0.020860  2.688561   \n",
       "1.02:1e-10   0.740608  36.491103 -0.107171  0.046395  0.020860  2.688561   \n",
       "1.03:1e-08   0.740608  36.491101 -0.107171  0.046395  0.020860  2.688561   \n",
       "1.04:0.0001  0.740608  36.471735 -0.107121  0.046362  0.020681  2.689258   \n",
       "1.05:0.001   0.740605  36.299120 -0.106679  0.046064  0.019094  2.695421   \n",
       "1.06:0.01    0.740344  34.724275 -0.102710  0.043381  0.005483  2.748085   \n",
       "1.07:1       0.635014  21.026665 -0.059368  0.017702 -0.072405  2.311548   \n",
       "1.08:5       0.415215  23.554065 -0.037414  0.012091 -0.054343  0.964243   \n",
       "1.09:10      0.297932  23.655470 -0.025734  0.008545 -0.038718  0.550144   \n",
       "1.1:20       0.191601  23.380079 -0.015848  0.005351 -0.024335  0.295135   \n",
       "\n",
       "                   NOX        RM       AGE       DIS       RAD       TAX  \\\n",
       "1.01:1e-15  -17.795759  3.804752  0.000751 -1.475759  0.305655 -0.012329   \n",
       "1.02:1e-10  -17.795759  3.804752  0.000751 -1.475759  0.305655 -0.012329   \n",
       "1.03:1e-08  -17.795757  3.804753  0.000751 -1.475759  0.305655 -0.012329   \n",
       "1.04:0.0001 -17.783439  3.805387  0.000739 -1.475111  0.305233 -0.012309   \n",
       "1.05:0.001  -17.673424  3.811020  0.000635 -1.469303  0.301487 -0.012127   \n",
       "1.06:0.01   -16.652015  3.860450 -0.000289 -1.413705  0.268783 -0.010573   \n",
       "1.07:1       -3.927643  2.874591 -0.009297 -0.249665 -0.004520 -0.002736   \n",
       "1.08:5       -2.661259  1.193735 -0.009043  0.018627 -0.026814 -0.002104   \n",
       "1.09:10      -1.943350  0.708549 -0.006844  0.037962 -0.021554 -0.001513   \n",
       "1.1:20       -1.243978  0.395494 -0.004452  0.032190 -0.014334 -0.000955   \n",
       "\n",
       "              PTRATIO         B     LSTAT  \n",
       "1.01:1e-15  -0.953464  0.009393 -0.525467  \n",
       "1.02:1e-10  -0.953464  0.009393 -0.525467  \n",
       "1.03:1e-08  -0.953464  0.009393 -0.525467  \n",
       "1.04:0.0001 -0.953268  0.009392 -0.525373  \n",
       "1.05:0.001  -0.951519  0.009390 -0.524538  \n",
       "1.06:0.01   -0.935255  0.009365 -0.516574  \n",
       "1.07:1      -0.535678  0.006218 -0.261501  \n",
       "1.08:5      -0.249294  0.003280 -0.111930  \n",
       "1.09:10     -0.155247  0.002176 -0.068882  \n",
       "1.1:20      -0.089635  0.001315 -0.039586  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame(coeffs).transpose()\n",
    "columns = ['SCORE', 'INTERCEPT']+list(X.columns)\n",
    "dict = {}\n",
    "i = 0 \n",
    "for c in columns:\n",
    "    dict[i] = c\n",
    "    i += 1\n",
    "a.rename(columns = dict, inplace=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the prior exercise demonstrates an increasing model score with a decreasing alpha, while on the other increasing likelihood of overfitting, how do we know what the optimal level of alpha is? We can do so using grid search with cross validation. This will enable us to test different levels of alpha using k-fold cross-validation, which should give us unbiased score estimates.  \n",
    "\n",
    "Once we perform that, we can see that grid search actually recommends and alpha of 1 as the optimal weight for our L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=1, copy_X=True, fit_intercept=True, max_iter=None, normalize=True,\n",
      "   random_state=None, solver='auto', tol=0.001)\n",
      "0.635014072122\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(Ridge(alpha=alpha, normalize=True), param_grid={'alpha': alpha_ridge}, cv=10, n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print grid_search.best_estimator_\n",
    "print grid_search.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 Regularization with LASSO Regression\n",
    "\n",
    "LASSO (Least Absolute Shrinkage and Selection Operator) regression relies on L1 regularization, which adds a factor of sum of absolute value of coefficients in the optimization objective. Thus, lasso regression optimizes the following:\n",
    "\n",
    "Objective = RSS + λ * (sum of absolute value of coefficients)\n",
    "\n",
    "<img src=\"extras/Lasso1.png\" width=\"700\" height=\"700\" />\n",
    "\n",
    "Here, λ (lambda, or alpha in sklearn) works similar to that of ridge and provides a trade-off between balancing RSS and magnitude of coefficients. Like that of ridge, λ can take various values. Lets iterate it here briefly:  \n",
    "\n",
    "λ = 0:  \n",
    "\n",
    "- Same coefficients as simple linear regression  \n",
    "\n",
    "λ = ∞:  \n",
    "\n",
    "- All coefficients zero (same logic as before)  \n",
    "\n",
    "0 < λ < ∞: \n",
    "\n",
    "- Coefficients between 0 and that of simple linear regression\n",
    "\n",
    "Because L1 relies one the absolute value of the coefficients, we aren't able to use gradient descent to optimize (meaning no learning rate). In this case, we have to use a different technique called as coordinate descent which is based on the concept of sub-gradients. In L1 regularization, our decision rule for a given weight is affected:\n",
    "\n",
    "<img src=\"extras/Lasso2.png\" width=\"700\" height=\"700\" />\n",
    "\n",
    "Here g(w-j) represents (but not exactly) the difference between actual outcome and the predicted outcome considering all EXCEPT the jth variable. If this value is small, it means that the algorithm is able to predict the outcome fairly well even without the jth variable and thus it can be removed from the equation by setting a zero coefficient. This gives us some intuition into why the coefficients become zero in case of lasso regression, especially once lambda starts increasing. \n",
    "\n",
    "In coordinate descent, checking convergence is another issue. Since gradients are not defined, we need an alternate method. Many alternatives exist but the simplest one is to check the step size of the algorithm. We can check the maximum difference in weights in any particular cycle over all feature weights.\n",
    "\n",
    "If this is lower than ‘tol’ specified, algorithm will stop. The convergence is not as fast as gradient descent and we might have to set the ‘max_iter’ parameter if a warning appears saying that the algorithm stopped before convergence. This is why we need to specify this parameter in the Lasso generic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO (Least Absolute Shrinkage and Selection Operator) Regression Coding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "alpha_ridge = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n",
    "\n",
    "coeffs = {}\n",
    "z = 1.01\n",
    "for alpha in alpha_ridge:\n",
    "    r = Lasso(alpha=alpha, normalize=True, max_iter=1e5)\n",
    "    r = r.fit(X, y)\n",
    "    coeffs[str(z)+':'+str(alpha)] = np.append(r.score(X,y), np.append(r.intercept_, r.coef_))\n",
    "    z += .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, when we look at the coefficients for the different values of alpha, what we see is that their magnitude for every predictor decreases as alpha increases. Starting at alpha=.001, we begin to see some cofficients drop to zero.  Interestingly, we don't see the model score drop too much as alpha increases. Additionally, starting with alpha=1, we see that every coefficient drops to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SCORE</th>\n",
       "      <th>INTERCEPT</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.01:1e-15</th>\n",
       "      <td>0.740608</td>\n",
       "      <td>36.491103</td>\n",
       "      <td>-0.107171</td>\n",
       "      <td>0.046395</td>\n",
       "      <td>0.020860</td>\n",
       "      <td>2.688561</td>\n",
       "      <td>-17.795759</td>\n",
       "      <td>3.804752</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>-1.475759</td>\n",
       "      <td>0.305655</td>\n",
       "      <td>-0.012329</td>\n",
       "      <td>-0.953464</td>\n",
       "      <td>0.009393</td>\n",
       "      <td>-0.525467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.02:1e-10</th>\n",
       "      <td>0.740608</td>\n",
       "      <td>36.491103</td>\n",
       "      <td>-0.107171</td>\n",
       "      <td>0.046395</td>\n",
       "      <td>0.020860</td>\n",
       "      <td>2.688561</td>\n",
       "      <td>-17.795758</td>\n",
       "      <td>3.804752</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>-1.475759</td>\n",
       "      <td>0.305655</td>\n",
       "      <td>-0.012329</td>\n",
       "      <td>-0.953464</td>\n",
       "      <td>0.009393</td>\n",
       "      <td>-0.525467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.03:1e-08</th>\n",
       "      <td>0.740608</td>\n",
       "      <td>36.491086</td>\n",
       "      <td>-0.107170</td>\n",
       "      <td>0.046395</td>\n",
       "      <td>0.020860</td>\n",
       "      <td>2.688562</td>\n",
       "      <td>-17.795742</td>\n",
       "      <td>3.804753</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>-1.475758</td>\n",
       "      <td>0.305655</td>\n",
       "      <td>-0.012329</td>\n",
       "      <td>-0.953463</td>\n",
       "      <td>0.009393</td>\n",
       "      <td>-0.525467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.04:0.0001</th>\n",
       "      <td>0.740603</td>\n",
       "      <td>36.313495</td>\n",
       "      <td>-0.106436</td>\n",
       "      <td>0.045930</td>\n",
       "      <td>0.017759</td>\n",
       "      <td>2.691376</td>\n",
       "      <td>-17.632310</td>\n",
       "      <td>3.810254</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>-1.471748</td>\n",
       "      <td>0.300900</td>\n",
       "      <td>-0.012090</td>\n",
       "      <td>-0.951010</td>\n",
       "      <td>0.009371</td>\n",
       "      <td>-0.524805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.05:0.001</th>\n",
       "      <td>0.740277</td>\n",
       "      <td>34.859822</td>\n",
       "      <td>-0.099716</td>\n",
       "      <td>0.042336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.692627</td>\n",
       "      <td>-16.543743</td>\n",
       "      <td>3.847766</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-1.416549</td>\n",
       "      <td>0.262504</td>\n",
       "      <td>-0.010242</td>\n",
       "      <td>-0.934032</td>\n",
       "      <td>0.009156</td>\n",
       "      <td>-0.523085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.06:0.01</th>\n",
       "      <td>0.719520</td>\n",
       "      <td>22.428998</td>\n",
       "      <td>-0.035814</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>2.354637</td>\n",
       "      <td>-8.567505</td>\n",
       "      <td>4.233876</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.743384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.819089</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>-0.521000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.07:1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.532806</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.08:5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.532806</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.09:10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.532806</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.1:20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.532806</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                SCORE  INTERCEPT      CRIM        ZN     INDUS      CHAS  \\\n",
       "1.01:1e-15   0.740608  36.491103 -0.107171  0.046395  0.020860  2.688561   \n",
       "1.02:1e-10   0.740608  36.491103 -0.107171  0.046395  0.020860  2.688561   \n",
       "1.03:1e-08   0.740608  36.491086 -0.107170  0.046395  0.020860  2.688562   \n",
       "1.04:0.0001  0.740603  36.313495 -0.106436  0.045930  0.017759  2.691376   \n",
       "1.05:0.001   0.740277  34.859822 -0.099716  0.042336  0.000000  2.692627   \n",
       "1.06:0.01    0.719520  22.428998 -0.035814  0.013100 -0.000000  2.354637   \n",
       "1.07:1       0.000000  22.532806 -0.000000  0.000000 -0.000000  0.000000   \n",
       "1.08:5       0.000000  22.532806 -0.000000  0.000000 -0.000000  0.000000   \n",
       "1.09:10      0.000000  22.532806 -0.000000  0.000000 -0.000000  0.000000   \n",
       "1.1:20       0.000000  22.532806 -0.000000  0.000000 -0.000000  0.000000   \n",
       "\n",
       "                   NOX        RM       AGE       DIS       RAD       TAX  \\\n",
       "1.01:1e-15  -17.795759  3.804752  0.000751 -1.475759  0.305655 -0.012329   \n",
       "1.02:1e-10  -17.795758  3.804752  0.000751 -1.475759  0.305655 -0.012329   \n",
       "1.03:1e-08  -17.795742  3.804753  0.000751 -1.475758  0.305655 -0.012329   \n",
       "1.04:0.0001 -17.632310  3.810254  0.000399 -1.471748  0.300900 -0.012090   \n",
       "1.05:0.001  -16.543743  3.847766 -0.000000 -1.416549  0.262504 -0.010242   \n",
       "1.06:0.01    -8.567505  4.233876 -0.000000 -0.743384  0.000000 -0.000000   \n",
       "1.07:1       -0.000000  0.000000 -0.000000  0.000000 -0.000000 -0.000000   \n",
       "1.08:5       -0.000000  0.000000 -0.000000  0.000000 -0.000000 -0.000000   \n",
       "1.09:10      -0.000000  0.000000 -0.000000  0.000000 -0.000000 -0.000000   \n",
       "1.1:20       -0.000000  0.000000 -0.000000  0.000000 -0.000000 -0.000000   \n",
       "\n",
       "              PTRATIO         B     LSTAT  \n",
       "1.01:1e-15  -0.953464  0.009393 -0.525467  \n",
       "1.02:1e-10  -0.953464  0.009393 -0.525467  \n",
       "1.03:1e-08  -0.953463  0.009393 -0.525467  \n",
       "1.04:0.0001 -0.951010  0.009371 -0.524805  \n",
       "1.05:0.001  -0.934032  0.009156 -0.523085  \n",
       "1.06:0.01   -0.819089  0.007275 -0.521000  \n",
       "1.07:1      -0.000000  0.000000 -0.000000  \n",
       "1.08:5      -0.000000  0.000000 -0.000000  \n",
       "1.09:10     -0.000000  0.000000 -0.000000  \n",
       "1.1:20      -0.000000  0.000000 -0.000000  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame(coeffs).transpose()\n",
    "columns = ['SCORE', 'INTERCEPT']+list(X.columns)\n",
    "dict = {}\n",
    "i = 0 \n",
    "for c in columns:\n",
    "    dict[i] = c\n",
    "    i += 1\n",
    "a.rename(columns = dict, inplace=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using grid search to find the optimal alpha, we get alpha=.01 as the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=0.01, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=True, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "0.719519645326\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(Lasso(alpha=alpha, normalize=True), param_grid={'alpha': alpha_ridge}, cv=10, n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print grid_search.best_estimator_\n",
    "print grid_search.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/\n",
    "- http://scikit-learn.org/stable/modules/linear_model.html#lasso\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso\n",
    "- http://machinelearningmastery.com/how-to-tune-algorithm-parameters-with-scikit-learn/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
