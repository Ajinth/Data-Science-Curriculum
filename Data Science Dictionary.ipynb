{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Data Science Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Accuracy measures the proportion of all the values that are correctly predicted.  \n",
    "ACC = (TP + TN)/(TP + TN + FP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Boosting (AdaBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomorative hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Linkeage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "See Bootstrap Aggregating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Aggregating\n",
    "[Bootstrap aggregating](https://en.wikipedia.org/wiki/Bootstrap_aggregating) is a machine learning ensemble technique designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Instead of using the same training dataset to fit our individual classifiers in the ensemble, we draw random samples with replacement. A individual sample is known as a bootstrap sample. If the size of the bootstrap sample is the same as the training set, then the bootstrap sample is expected to have the fraction (1 - 1/e)(≈63.2%) of the unique examples of D, the rest being duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cardinal Number\n",
    "A number whose value is a measure of a given quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central Limit Theorem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi Square Test of Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete linkage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks (CNNs or ConvNets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "The [correlation coefficient](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient), also known as the Pearson product-moment correlation coefficient (PPMCC or PCC or Pearson's r), is a measure of the linear dependence between two variables X and Y, giving a value between +1 and −1 inclusive, where 1 is total positive linear correlation, 0 is no linear correlation, and −1 is total negative linear correlation.\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/f76ccfa7c2ed7f5b085115086107bbe25d329cec)\n",
    "\n",
    "If X and Y are independent random variables, then the covariance and correlation between them is zero. However, the inverse of this is not necessarily true: just because the covariance or correlation between two random variables is zero does not mean that they are necessarily independent. Therem maybe a more complicated nonlinear association such as X^2 + Y^2 = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance\n",
    "\n",
    "[Covariance](https://en.wikipedia.org/wiki/Covariance#Calculating_the_sample_covariance) is a measure of the joint variability of two random variables change.  If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, i.e., the variables tend to show similar behavior, the covariance is positive. The sign of the covariance shows the tendency of the linear relationship between two variables. \n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/7331bb9b6e36128d1d9cb735b11b65427929105d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curse of dimensionality\n",
    "A phenomenon where the feature space becomes increasingly sparse for an increasing number of dimensions of a fixed-size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision boundary\n",
    "Allows us to separate classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendrograms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density-based Spatial Clustering of Applications wiht Noise (DBSCAN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction\n",
    "A technique of compressing data onto a smaller dimensional space while retaining most of the relevant information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions\n",
    "Features or explanatory variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisive hierarchical clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Product\n",
    "\n",
    "A [dot product](https://en.wikipedia.org/wiki/Dot_product) (sometimes referred to as the scalar product or the inner product) is an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number. \n",
    "\n",
    "Example: the dot product of vectors [1, 3, −5] and [4, −2, −1] is:  \n",
    "(1)(4) + (3)(-2) + (-5)(-1)  \n",
    "= 4 - 6 + 5  \n",
    "= 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods\n",
    "Also know as ensemble learning, [ensemble methods](https://en.wikipedia.org/wiki/Ensemble_learning) use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the individual learning algorithms alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epochs\n",
    "Passes over the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance\n",
    "The [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) is the ordinary (straight line) distance between two points. In Cartesian coordinates, the Euclidian distance between points p and q in Euclidean n-space is given by the Pythagorean formula:\n",
    "![alt](extras/EuclideanDistanceN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate\n",
    "The [false positive rate](https://en.wikipedia.org/wiki/False_positive_rate) is the ratio between the number of negative events wrongly categorized as positive (false positives) and the total number of actual negative events (regardless of classification). It is mathematically equivalent to the type I error rate, although the type I error rate is generally associated with the a-priori setting of the significance level, while the false positive rate is associated with a post-priori result.  \n",
    "FPR = FP/N = FP/(FP + TN) = 1 - SPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy C-menas algorithm (FCM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization error\n",
    "The error associated with moving from a training data set to a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat Map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Parameters that are not learned from the data but represent the knobs of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain\n",
    "In the context of decision trees, information gain is defined as the expected reduction in entropy caused by partitioning the examples according to a given attribute. Information gain can be formalized as\n",
    "![alt](extras/IG.png)\n",
    "Here, f is the feature to perform the split, Dp and Dj are the datasets of the parent and the jth child node, I is our impurity measure, Np is the total number of samples at the parent node, and Nj is the number of samples in the jth child node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance-based learning\n",
    "Models characterized by memorizing the training dataset (lazy learning, like the KNN algorithm, is a special case of instance-based learning that is associated with no (zero) cost during the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse document frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO (Least Absolute Shrinkage and Selection Operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy learner\n",
    "An algorithm that doesn't learn a discriminative function from the training data but memorizes the trainig dataset instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Absolute Shrinkage and Selection Operator (LASSO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmitization\n",
    "A technique for obtaining the canonincal (grammatically correct) forms of individual words, the co-called lemmas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)\n",
    "See corresponding module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit function\n",
    "See corresponding module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misclassifcation error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nominal vs ordinal features\n",
    "In the context of categorical features, nominal values or observations are ones that can be assigned a code in the form of a number where the numbers are simply labels (they can be counted but not ordered). Ordinal values or observations can be ranked or have a rating scale attached. We can count _and_ order ordinal features, but not measure them. Depending on the situation, ordinal variables can be treated as categorical or as quantitative variables. Nominal values are not to be confused with cardinal numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonparametric models\n",
    "Nonparametric models are algorithms that do not make strong assumptions about the form of the mapping function. The parameters are usually said to be infinite in dimensions and so can express the characteristics in the data much better than parametric models. In the non-parametric approach, all of the previously used training data need to used along with the untrained values and the estimated quantities in order to perform a prediction. Examples include k-nearest neighbors and support vector machines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odds ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "A process of creating dummy features for each unique value in the nominal feature column (it returns a sparse matrix using the transform menthod)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of core learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric models\n",
    "Parametric models assume a finite set of parameters that can be estimated and used to represent the functional form of the underlying data. Examples include logistic and linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson product-moment correlation coefficients\n",
    "Measures the linear dependence between pairs of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "Also known as the positive predictive value, precision measures the proportion of predicted positive values that are correct.  \n",
    "PPV = TP/(TP + FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Curves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "See corresponding module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype-based clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression\n",
    "See corresponding module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sample Consensus (RANSAC) algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "Also known as the true positive rate or sensitivity, recall measures the proportion of positives that are correctly identified as such. TPR = TP/P = TP/(TP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operator Characteristic (ROC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "The introduction of additional information (bias) to penalize extreme parameter weights (L1 & L2 regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response variable\n",
    "Outcome variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity\n",
    "Also known as the true positive rate or recall, sensitivity measures the proportion of positives that are correctly identified as such.  \n",
    "TPR = TP/P = TP/(TP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Backward Selection (SBS)\n",
    "A technique of dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sillhouette plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single linkage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse vector\n",
    "A vector made up of mostly zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specificity\n",
    "Also known as the true negative rate, specificity measures the proportion of negatives that are correctly identified as such.  \n",
    "SPC = TN/N = TN/(TN+FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation\n",
    "\n",
    "[Standard deviation](https://en.wikipedia.org/wiki/Standard_deviation) is used to quantify the amount of variation or dispersion of a set of data values.[1] A low standard deviation indicates that the data points tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the data points are spread out over a wider range of values. Standard deviation is the square root of variance, and it has the advantage of being in the same units of measure as teh random variable. \n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/6d5e98f44504dd173af145ff80cda7cfd80d3050)\n",
    "\n",
    "In the image below, dark blue is one standard deviation on either side of the mean. For the normal distribution, this accounts for 68.27 percent of the set; while two standard deviations from the mean (medium and dark blue) account for 95.45 percent; three standard deviations (light, medium, and dark blue) account for 99.73 percent; and four standard deviations account for 99.994 percent. The two points of the curve that are one standard deviation from the mean are also the inflection points.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Standard_deviation_diagram.svg/350px-Standard_deviation_diagram.svg.png)\n",
    "\n",
    "It is very important to note that the standard deviation of a population and the standard error of a statistic derived from that population (such as the mean) are quite different but related (related by the inverse of the square root of the number of observations). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop-words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Negative Rate\n",
    "Also known as specificity, specificity measures the proportion of negatives that are correctly identified as such.  \n",
    "SPC = TN/N = TN/(TN+FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positive Rate\n",
    "Also known as the sensitivity or recall, sensitivity measures the proportion of positives that are correctly identified as such.  \n",
    "TPR = TP/P = TP/(TP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance\n",
    "\n",
    "Variance (sigma squared) is the average squared difference between a random variable and its mean value. The larger the variance of a random variable, the more spread out the values of the random variable are. \n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/55622d2a1cf5e46f2926ab389a8e3438edb53731)\n",
    "\n",
    "Alternatively:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/86060a001d3617ca85ed73efb63ec7908d77fa5f)\n",
    "\n",
    "Variance can also be thought of as the covariance of a variable with itself:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/a6df8498bba86383d7d165590f515b929746f243)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ward's Linkage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak learners\n",
    "A simple base classifier, such as a tree stump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word stemming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
