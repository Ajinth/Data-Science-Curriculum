{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Data Science Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Accuracy measures the proportion of all the values that are correctly predicted.  \n",
    "ACC = (TP + TN)/(TP + TN + FP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Boosting (AdaBoost)\n",
    "\n",
    "In the [AdaBoost algorithm](https://en.wikipedia.org/wiki/AdaBoost), the output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomorative Hierarchical Clustering\n",
    "\n",
    "[Agglomerative hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering) is a \"bottom up\" approach where each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANCOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Linkeage\n",
    "\n",
    "Average linkage is an algorithm used for agglomerative hierarchical clustering where we merge the cluster pairs based on the minimum average distances between all group members in the two clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "See Bootstrap Aggregating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bessel's Correction  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias - Variance Tradeoff\n",
    "\n",
    "The [bias–variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) is a central problem in supervised learning. Ideally, one wants to choose a model that both accurately captures the regularities in its training data, but also generalizes well to unseen data. Unfortunately, it is typically impossible to do both simultaneously. High-variance learning methods may be able to represent their training set well, but are at risk of overfitting to noisy or unrepresentative training data. In contrast, algorithms with high bias typically produce simpler models that don't tend to overfit, but may underfit their training data, failing to capture important regularities.  \n",
    "\n",
    "Models with low bias are usually more complex (e.g. higher-order regression polynomials), enabling them to represent the training set more accurately. In the process, however, they may also represent a large noise component in the training set, making their predictions less accurate - despite their added complexity. In contrast, models with higher bias tend to be relatively simple (low-order or even linear regression polynomials), but may produce lower variance predictions when applied beyond the training set.  \n",
    "\n",
    "To define our key terms:  \n",
    "\n",
    "[Bias](https://en.wikipedia.org/wiki/Bias_of_an_estimator) is error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). It can also be thought of as the error within the training dataset.  \n",
    "\n",
    "Variance is error that results from fluctuations in the data when going from the training set to test set (usually caused by overfitting). It can be thought of as the difference between the testing error and the training error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Aggregating\n",
    "[Bootstrap aggregating](https://en.wikipedia.org/wiki/Bootstrap_aggregating) is a machine learning ensemble technique designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Instead of using the same training dataset to fit our individual classifiers in the ensemble, we draw random samples with replacement. A individual sample is known as a bootstrap sample. If the size of the bootstrap sample is the same as the training set, then the bootstrap sample is expected to have the fraction (1 - 1/e)(≈63.2%) of the unique examples of D, the rest being duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box-Cox Transformation  \n",
    "\n",
    "[The Box-Cox Transformation](http://www.itl.nist.gov/div898/handbook/eda/section3/boxcoxno.htm) is a transformation technique which results in an approximately normal distribution of data. It is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\Large y_i^{(\\lambda)} = \\left\\{ {\\frac{y_i^\\lambda-1}{\\lambda}\\text{ if } \\lambda \\neq 0,\\atop ln(y_i) \\text{ if } \\lambda = 0}\\right.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above holds true only for datasets where the values are greater than zero (see [here](https://en.wikipedia.org/wiki/Power_transform#Box.E2.80.93Cox_transformation) for alternatives).  \n",
    "\n",
    "We can identify the proper value of lambda using the Box-Cox plots, which show the correlation between our transformed data and the normal probability plot for different levels of lambda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cardinal Number\n",
    "A number whose value is a measure of a given quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central Limit Theorem\n",
    "\n",
    "The [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) establishes that given certain conditions, the arithmetic mean of a sufficiently large number of iterates of independent random variables, each with a well-defined (finite) expected value and finite variance, will be approximately normally distributed, regardless of the underlying distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Square Goodness of Fit Test  \n",
    "\n",
    "http://www.itl.nist.gov/div898/handbook/eda/section3/eda35f.htm\n",
    "and this\n",
    "https://learn.bu.edu/bbcswebdav/pid-826908-dt-content-rid-2073693_1/courses/13sprgmetcj702_ol/week05/metcj702_W05S02T05_limitations.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Square Test of Independence\n",
    "\n",
    "Also known as [Pearson's chi-squared test](https://en.wikipedia.org/wiki/Pearson's_chi-squared_test), the chi-square test of independence is applied when you have two categorical variables from a single population. It is used to determine whether there is a significant association between the two variables. The hypothesis is set up as:\n",
    "\n",
    "H0: Variable A and Variable B are independent.  \n",
    "Ha: Variable A and Variable B are not independent.\n",
    "\n",
    "The test procedure described in this lesson is appropriate when the following conditions are met:\n",
    "\n",
    "- The sampling method is simple random sampling.\n",
    "- The variables under study are each categorical.\n",
    "- If sample data are displayed in a contingency table, the expected frequency count for each cell of the table is at least 5.\n",
    "\n",
    "A good example can be found [here](http://stattrek.com/chi-square-test/independence.aspx?Tutorial=AP). \n",
    "\n",
    "It should be noted that the chi-squared test of independence and the test of equal proportions are [identical under certain conditions](http://stats.stackexchange.com/questions/2391/what-is-the-relationship-between-a-chi-squared-test-and-test-of-equal-proportion). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohen's D  \n",
    "\n",
    "https://en.wikiversity.org/wiki/Cohen%27s_d\n",
    "http://rpsychologist.com/d3/cohend/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete linkage\n",
    "\n",
    "[Complete linkage](https://en.wikipedia.org/wiki/Complete-linkage_clustering) is a method for agglomerative hierarchical clustering. At the beginning of the process, each element is in a cluster of its own. The clusters are then sequentially combined into larger clusters until all elements end up being in the same cluster. At each step, the two clusters separated by the shortest distance are combined. The definition of 'shortest distance' is what differentiates between the different agglomerative clustering methods. In complete-linkage clustering, the link between two clusters contains all element pairs, and the distance between clusters equals the distance between those two elements (one in each cluster) that are farthest away from each other. The shortest of these links that remains at any step causes the fusion of the two clusters whose elements are involved. The method is also known as farthest neighbour clustering. The result of the clustering can be visualized as a dendrogram, which shows the sequence of cluster fusion and the distance at which each fusion took place.\n",
    "\n",
    "![](extras/linkage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugate Priors  \n",
    "\n",
    "The concept of the [conjugate priors](https://en.wikipedia.org/wiki/Conjugate_prior) states that if the prior probability distributions are in the same family as the posterior distributions, then they are known as conjugate distributions, and the prior is known as the conjugate prior to the likelihood function. In simpler terms, if we know the distribution of the likelihood function, we can determine the distribution of the posterior and prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Divergence  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks (CNNs or ConvNets)\n",
    "\n",
    "For details, see module on [convolutional neural networks using Tensorflow](https://github.com/zlatankr/Data-Science-Curriculum/blob/master/Deep%20Learning%20Convolution%20in%20TensorFlow.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation coefficient   \n",
    "\n",
    "The [correlation coefficient](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient), also known as the Pearson product-moment correlation coefficient (PPMCC or PCC or Pearson's r), is a measure of the linear dependence between two variables X and Y, giving a value between +1 and −1 inclusive, where 1 is total positive linear correlation, 0 is no linear correlation, and −1 is total negative linear correlation.\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/f76ccfa7c2ed7f5b085115086107bbe25d329cec)\n",
    "\n",
    "If X and Y are independent random variables, then the covariance and correlation between them is zero. However, the inverse of this is not necessarily true: just because the covariance or correlation between two random variables is zero does not mean that they are necessarily independent. There may be a more complicated nonlinear association such as X^2 + Y^2 = 1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "A [cost function](https://en.wikipedia.org/wiki/Loss_function), also known as a loss function, is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event. An optimization problem seeks to minimize the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance\n",
    "\n",
    "[Covariance](https://en.wikipedia.org/wiki/Covariance) is a measure of the joint variability of two random variables.  If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, i.e., the variables tend to show similar behavior, the covariance is positive. The sign of the covariance shows the tendency of the linear relationship between two variables. \n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/7331bb9b6e36128d1d9cb735b11b65427929105d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If X and Y are independent, then their covariance is zero. This follows because under independence,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\ E[XY] = E[X]\\cdot E[Y]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magnitude of covariance can be difficult to interpret, so we often normalize it by converting it to the correlation coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy  \n",
    "\n",
    "[Cross entropy](https://en.wikipedia.org/wiki/Cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curse of dimensionality\n",
    "\n",
    "The [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality#Machine_learning) is a phenomenon where the feature space becomes increasingly sparse for an increasing number of dimensions of a fixed-size (that is, the density of the training samples becomes very low). Most notably, the predictive power of our models diminishes as the dimensionality increases. Adding the additional dimensions to obtain perfect classification results corresponds to using a complicated non-linear classifier in the lower dimensional feature space. As a result, the classifier learns the appearance of specific instances and exceptions of our training dataset. Because of this, the resulting classifier would fail on real-world data, consisting of an infinite amount of target labels that often do not adhere to these exceptions. The amount of training data needs to grow exponentially fast to maintain the same coverage and to avoid overfitting.  \n",
    "\n",
    "Additionally, a highly dimensional feature space degrades the effectiveness of distance metrics, because when the dimensionality of the feature space goes to infinity, the ratio of the difference in minimum and maximum Euclidean distance from sample point to the centroid, and the minimum distance itself, tends to zero. A clear example can be found [here](http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Davies-Bouldin Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision boundary\n",
    "Allows us to separate classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Belief Networks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendrograms\n",
    "\n",
    "A [dendrogram](https://en.wikipedia.org/wiki/Dendrogram) (from Greek dendro \"tree\" and gramma \"drawing\") is a tree diagram frequently used to illustrate the arrangement of the clusters produced by hierarchical clustering. The height of each node in the plot is proportional to the value of the intergroup dissimilarity between its two daughters.\n",
    "\n",
    "![](extras/dendrogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density-based Spatial Clustering of Applications with Noise (DBSCAN)\n",
    "\n",
    "[Density-based Spatial Clustering of Applications with Noise (DBSCAN)](https://en.wikipedia.org/wiki/DBSCAN) is a density-based clustering algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature.\n",
    "\n",
    "One of the main advantages of using DBSCAN is that it does not assume that the clusters have a spherical shape as in k-means. Furthermore, DBSCAN is different from k-means and hierarchical clustering in that it doesn't necessarily assign each point to a cluster but is capable of removing noise points.\n",
    "\n",
    "However, we should also note some of the disadvantages of DBSCAN. With an increasing number of features in our dataset—given a fixed size training set—the negative effect of the curse of dimensionality increases. This is especially a problem if we are using the Euclidean distance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic Algorithm\n",
    "\n",
    "A [deterministic algorithm](https://en.wikipedia.org/wiki/Deterministic_algorithm) is an algorithm which, given a particular input, will always produce the same output, with the underlying machine always passing through the same sequence of states. Non-deterministic algorithms have any of the following characteristics:\n",
    "\n",
    "- If it uses external state other than the input, such as user input, a global variable, a hardware timer value, a random value, or stored disk data.\n",
    "- If it operates in a way that is timing-sensitive, for example if it has multiple processors writing to the same data at the same time. In this case, the precise order in which each processor writes its data will affect the result.\n",
    "- If a hardware error causes its state to change in an unexpected way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction\n",
    "A technique of compressing data onto a smaller dimensional space while retaining most of the relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions\n",
    "Features or explanatory variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisive hierarchical clustering\n",
    "\n",
    "In divisive hierarchical clustering, we start with one cluster that encompasses all our samples, and we iteratively split the cluster into smaller clusters until each cluster only contains one sample. Because there exist O(2^n) ways of splitting each cluster, heuristics are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Product\n",
    "\n",
    "A [dot product](https://en.wikipedia.org/wiki/Dot_product) (sometimes referred to as the scalar product or the inner product) is an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number. \n",
    "\n",
    "Example: the dot product of vectors [1, 3, −5] and [4, −2, −1] is:  \n",
    "(1)(4) + (3)(-2) + (-5)(-1)  \n",
    "= 4 - 6 + 5  \n",
    "= 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues\n",
    "\n",
    "An [eigenvalue](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) is the magnitude of the scalar transformation of an eigenvector. In the eigenequation \n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/4c03cf705786a3d5f73ca46df287ae6739082160)\n",
    "\n",
    "where λ is a scalar in the field F, known as the eigenvalue, characteristic value, or characteristic root associated with the eigenvector v."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvectors\n",
    "\n",
    "An [eigenvector](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) of a linear transformation is a non-zero vector that does not change its direction when that linear transformation is applied to it. It is essentially a vector that has been scaled up by a transformation. In data analysis, we usually calculate the eigenvectors for a correlation or covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Method\n",
    "\n",
    "In statistics and, in particular, in the fitting of linear or logistic regression models, the [elastic net](https://en.wikipedia.org/wiki/Elastic_net_regularization) is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Method\n",
    "\n",
    "The <a href=\"https://en.wikipedia.org/wiki/Elbow_method_(clustering)\">elbow method</a> is a technique used in cluster analysis to help find the optimal number of clusters. This method looks at the percentage of variance explained (or sometimes a cost function, such as SSE) as a function of the number of clusters: one should choose a number of clusters so that adding another cluster doesn't give much better modeling of the data. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the \"elbow criterion\".\n",
    "\n",
    "An example can be found in the k-means clustering module, [here](https://github.com/zlatankr/Data-Science-Curriculum/blob/master/K-Means%20Clustering.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods\n",
    "Also known as ensemble learning, [ensemble methods](https://en.wikipedia.org/wiki/Ensemble_learning) use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the individual learning algorithms alone. This can be illustrated with a mathematical example. If we have _n_ number of models, each with an equal chance of making an incorrect prediction error _e_, the probability of the error happening _k_ times can be approximated by the binomial distribution:\n",
    "\n",
    "![](extras/ensemble1.png)\n",
    "\n",
    "To illustrate this with a more concrete example: if we have 11 base classifiers, each with an error of .25, the probability that the error wins in a majority vote classifier is\n",
    "\n",
    "![](extras/ensemble2.png)\n",
    "\n",
    "This illustrates that the error of the ensemble classifier is significantly lower than that of the individual models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epochs\n",
    "Passes over the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon Greedy\n",
    "\n",
    "Epsilon Greedy is an algorithm used to solve the [multi-armed bandit](https://en.wikipedia.org/wiki/Multi-armed_bandit) problem. The algorithm has a fixed rate of exploration, and it chooses to exploit the rest of the time. In psuedo code:   \n",
    "`eps = 0.1`  \n",
    "`while True:`  \n",
    "`   r = rand()`  \n",
    "`    if r < eps:`  \n",
    "`        # explore`  \n",
    "`        who random advertisement`  \n",
    "`    else:`  \n",
    "`        # exploit`  \n",
    "`        show best advertisement (as determined by #clicks/#impressions)`  \n",
    "\n",
    "One drawback of the Epsilon Greedy algorithm is that it will continue to do the same thing forever; even once one option has statistically proven to be better, the other option will still occasionally get chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance\n",
    "The [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) is the ordinary (straight line) distance between two points. In Cartesian coordinates, the Euclidian distance between points p and q in Euclidean n-space is given by the Pythagorean formula:\n",
    "![alt](extras/EuclideanDistanceN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation Maximization  \n",
    "\n",
    "_need to write something here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score  \n",
    "\n",
    "The [F1 score](https://en.wikipedia.org/wiki/F1_score) (also F-score or F-measure) is a measure of a test's accuracy. It considers both the precision p and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results, and r is the number of correct positive results divided by the number of positive results that should have been returned. The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst at 0.\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/7d63c1f5c659f95b5dfe5893213cc8ea7f8bea0a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Negative Rate  \n",
    "\n",
    "The false negative rate is the ratio between the number of positive events wrongly categorized as negative (false negative) and the total number of actual positive events (regardless of classification). It is mathematically equivalent to the Type II error rate, although the Type II error rate is generally associated with the a-priori setting of the Beta level, while the false negative rate is associated with a post-priori result.  \n",
    "\n",
    "FNR = FN/P = FN/(FN + TP) = 1 - Sensitivity or 1 - Power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate\n",
    "The [false positive rate](https://en.wikipedia.org/wiki/False_positive_rate) is the ratio between the number of negative events wrongly categorized as positive (false positives) and the total number of actual negative events (regardless of classification). It is mathematically equivalent to the Type I error rate, although the Type I error rate is generally associated with the a-priori setting of the significance level, while the false positive rate is associated with a post-priori result.  \n",
    "\n",
    "FPR = FP/N = FP/(FP + TN) = 1 - Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction  \n",
    "\n",
    "In the context of dimensionality reduction, feature extraction can be understood as an approach to data compression with the goal of maintaining most of the relevant information. Feature extraction is typically used to improve computational efficiency but can also help to reduce the curse of dimensionality—especially if we are working with nonregularized models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling\n",
    "\n",
    "[Feature scaling](https://en.wikipedia.org/wiki/Feature_scaling) is a method used to standardize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step. Examples include rescaling:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/98f129c35917ecf0996f1bf512b3f6ac8d01f19b)\n",
    "\n",
    "Standardization:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/b0aa2e7d203db1526c577192f2d9102b718eafd5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy C-means algorithm (FCM)\n",
    "\n",
    "Also called the fuzzy k-means or soft k-means, in the [fuzzy C-means algorithm](https://www.quora.com/What-is-the-difference-between-K-Means-and-Fuzzy-C-Means-Clustering) each point has a probability of belonging to each cluster, rather than completely belonging to just one cluster as it is the case in the traditional k-means. Fuzzy k-means specifically tries to deal with the problem where points are somewhat in between centers or otherwise ambiguous by replacing distance with probability, which of course could be some function of distance, such as having probability relative to the inverse of the distance. Fuzzy k-means uses a weighted centroid based on those probabilities. Processes of initialization, iteration, and termination are the same as the ones used in k-means. The resulting clusters are best analyzed as probabilistic distributions rather than a hard assignment of labels.  One should realize that k-means is a special case of fuzzy k-means when the probability function used is simply 1 if the data point is closest to a centroid and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization error\n",
    "The error associated with moving from a training data set to a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative models  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric series  \n",
    "\n",
    "A [geometric series](https://en.wikipedia.org/wiki/Geometric_series) is a series with a constant ratio between successive terms. For example, 1/2 + 1/4 + 1/8 + 1/16 + ... is geometric because each successive term can be obtained by multiplying the previous term by 1/2. The sum of a geometric series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\ a + ar + ar^2 + ar^3 + ar^4 + \\cdots \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{a}{1-r}, \\text{ for } \\mid r \\mid<1.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs sampling  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat Map\n",
    "\n",
    "A [heat map](https://en.wikipedia.org/wiki/Heat_map) (or heatmap) is a graphical representation of data where the individual values contained in a matrix are represented as colors.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Heatmap.png/800px-Heatmap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering\n",
    "\n",
    "[Hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering) (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types: agglomerative (bottom-up) and divisive (top-down). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Parameters that are not learned from the data but represent the knobs of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent Component Analysis\n",
    "\n",
    "In signal processing, [independent component analysis (ICA)](https://en.wikipedia.org/wiki/Independent_component_analysis) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that the subcomponents are non-Gaussian signals and that they are statistically independent from each other. ICA is a special case of blind source separation. A common example application is the \"cocktail party problem\" of listening in on one person's speech in a noisy room.\n",
    "\n",
    "An important note to consider is that if N sources are present, at least N observations (e.g. microphones) are needed to recover the original signals. This constitutes the square case (J = D, where D is the input dimension of the data and J is the dimension of the model). Other cases of underdetermined (J > D) and overdetermined (J < D) have been investigated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain\n",
    "In the context of decision trees, information gain is defined as the expected reduction in entropy caused by partitioning the examples according to a given attribute. Information gain can be formalized as\n",
    "![alt](extras/IG.png)\n",
    "Here, f is the feature to perform the split, Dp and Dj are the datasets of the parent and the jth child node, I is our impurity measure, Np is the total number of samples at the parent node, and Nj is the number of samples in the jth child node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance-based learning\n",
    "Models characterized by memorizing the training dataset (lazy learning, like the KNN algorithm, is a special case of instance-based learning that is associated with no (zero) cost during the learning process)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse document frequency\n",
    "\n",
    "The second part of the [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Term_frequency) calculation, the inverse document frequency is a measure of how much information the word provides, that is, whether the term is common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word, obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient.  \n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/ac67bc0f76b5b8e31e842d6b7d28f8949dab7937)\n",
    "\n",
    "where `N` is the total number of documents `d` in the corpus `D` and the denominator is the number of documents where the term `t` appears. If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator by adding a `1 +` to the term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means  \n",
    "\n",
    "[K-Means](https://en.wikipedia.org/wiki/K-means_clustering) is an unsupervised clustering algorithm that splits observations into _k_ units by grouping data around centroids that minimize a 'sum of squared errors' metric. The centroids are defined as the mean of each cluster and are known as the prototype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors  \n",
    "\n",
    "[The K-Nearest Neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) is a non-parametric classification algorithm that predicts the class of the test data by picking the nearest _k_ training observations and taking a majority vote of those training labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kullback-Leibler Divergence  \n",
    "\n",
    "Derived from the formula of entropy, [Kullback-Leibler Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) is a measure of information loss while approximating one distribution with another. It can be expressed as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "D_{KL}(p||q) = \\sum_{i=1}^{N} p(x_i)\\cdot (\\text{log }p(x_i) - \\text{log }q(x_i))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we think in terms of $log_2$, as is customary in the entropy equation, we can interpret this as \"how many bits of information we expect to lose\". We could rewrite our formula in terms of expectation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "(p∣∣q)=E[log p(x)−log q(x)]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, most commonly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "D_{KL}(p||q) = \\sum_{i=1}^{N} p(x_i)\\cdot log\\frac{p(x_i)}{q(x_i)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to not that KL Divergence is not symmetric, meaning that it has to be recalculated if we switch our 'p' and 'q' distributions.  \n",
    "\n",
    "One common way to use KK Divergence is to use the metric to find the optimal parameters of the approximating distribution. For example, if we are trying to approximate our data with the binomial distribution, we can plot KL Divergence for different values of $p$ and choose the lowest point in our graph as the best $p$.  \n",
    "\n",
    "Great tutorial here: https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace Smoothing\n",
    "\n",
    "In statistics, [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing), or additive smoothing, or Lidstone smoothing, is a technique used to smooth categorical data. Given an observation x = (x1, …, xd) from a multinomial distribution with N trials and parameter vector θ = (θ1, …, θd), a \"smoothed\" version of the data gives the estimator:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/78d9df669c92aa18fa0835aba09ee74c3d08b839)\n",
    "\n",
    "where the pseudocount α > 0 is the smoothing parameter (α = 0 corresponds to no smoothing). Additive smoothing is a type of shrinkage estimator, as the resulting estimate will be between the empirical estimate xi / N, and the uniform probability 1/d. Using Laplace's rule of succession, some authors have argued that α should be 1 (in which case the term add-one smoothing is also used), though in practice a smaller value is typically chosen.\n",
    "\n",
    "Additive smoothing is commonly a component of Naive Bayes classifiers.\n",
    "\n",
    "In a bag of words model of natural language processing and information retrieval, the data consists of the number of occurrences of each word in a document. Additive smoothing allows the assignment of non-zero probabilities to words which do not occur in the sample. Recent studies have proven that additive smoothing is more effective than other probability smoothing methods in several retrieval tasks such as language-model-based pseudo-relevance feedback and recommender systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation\n",
    "\n",
    "In natural language processing, [latent Dirichlet allocation (LDA)](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy learner\n",
    "An algorithm that doesn't learn a discriminative function from the training data but memorizes the trainig dataset instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve\n",
    "\n",
    "A [learning curve](http://scikit-learn.org/stable/modules/learning_curve.html) shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error. If both the validation score and the training score converge to a value that is too low with increasing size of the training set, we will not benefit much from more training data. In the following plot you can see an example: naive Bayes roughly converges to a low score.  \n",
    "\n",
    "<img src=\"http://scikit-learn.org/stable/_images/sphx_glr_plot_learning_curve_0011.png\" style=\"width:600px;height:450px;\">\n",
    "\n",
    "A [learning curve](http://stackoverflow.com/questions/4617365/what-is-a-learning-curve-in-machine-learning) also allows you to find the point from which the algorithm starts to learn. If you take a curve and then slice a slope tangent for derivative at the point that it starts to reach constant is when it starts to build its learning ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Absolute Shrinkage and Selection Operator (LASSO)\n",
    "\n",
    "<a href=\"https://en.wikipedia.org/wiki/Lasso_(statistics)\">Least absolute shrinkage and selection operator (LASSO)</a> is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces.\n",
    "\n",
    "For a detailed example, see [LASSO module](https://github.com/zlatankr/Data-Science-Curriculum/blob/master/Regularization%20-%20LASSO%20and%20Ridge%20Regression.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "A technique for obtaining the canonincal (grammatically correct) forms of individual words, the co-called lemmas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)\n",
    "\n",
    "[Linear discriminant analysis (LDA)](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition, and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before classification.\n",
    "\n",
    "For more info, see corresponding [module](https://github.com/zlatankr/Data-Science-Curriculum/blob/master/Linear%20Discriminant%20Analysis.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearity of Expectation  \n",
    "\n",
    "[Linearity of expectation](https://brilliant.org/wiki/linearity-of-expectation/) is the property that the expected value of the sum of random variables is equal to the sum of their individual expected values, regardless of whether they are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\ E[X+Y] = E[X] + E[Y]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression  \n",
    "\n",
    "The logistic regression model is simply a non-linear transformation of the linear regression, using a sigmoid, thereby changing the interpretation of the coefficients and output to be the log-odds ratio. For more, see appropriate [module](https://github.com/zlatankr/Data-Science-Curriculum/blob/master/Logistic%20Regression.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit Function\n",
    "\n",
    "The logit function is the inverse of the sigmoidal \"logistic\" function, or logistic transform, used in mathematics, especially in statistics. When the function's parameter represents a probability p, the logit function gives the log-odds, or the logarithm of the odds p/(1 − p).\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/1c11d7f12346281c289cbfec8c6e75f06caf298e)\n",
    "\n",
    "See also the logistic model [module](https://github.com/zlatankr/Data-Science-Curriculum/blob/master/Logistic%20Regression.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "See cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manhattan Distance\n",
    "\n",
    "Also known as the taxicab metric, the [Manhattan distance](https://en.wikipedia.org/wiki/Taxicab_geometry) is a metric in which the distance between two points is the sum of the absolute differences of their Cartesian coordinates.\n",
    "\n",
    "The taxicab distance, `d1`, between two vectors `p`, `q`  in an n-dimensional real vector space with fixed Cartesian coordinate system, is the sum of the lengths of the projections of the line segment between the points onto the coordinate axes. More formally,\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/02436c34fc9562eb170e2e2cfddbb3303075b28e)\n",
    "\n",
    "where `(p, q)` are vectors\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/3be69c76d8560e245117031391182ca0cd95130d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Margin of Error  \n",
    "\n",
    "http://stattrek.com/estimation/margin-of-error.aspx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Chain\n",
    "\n",
    "In probability theory and statistics, a [Markov chain](https://en.wikipedia.org/wiki/Markov_chain#The_general_case), named after the Russian mathematician Andrey Markov, is a stochastic process that satisfies the Markov property (usually characterized as \"memorylessness\"). Loosely speaking, a process satisfies the Markov property if one can make predictions for the future of the process based solely on its present state just as well as one could knowing the process's full history. i.e., conditional on the present state of the system, its future and past are independent.\n",
    "\n",
    "In discrete time, the process is known as a discrete-time Markov chain (DTMC). It undergoes transitions from one state to another on a state space, with the probability distribution of the next state depending only on the current state and not on the sequence of events that preceded it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation\n",
    "\n",
    "In statistics, [maximum likelihood estimation (MLE)](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) is a method of estimating the parameters of a statistical model given observations, by finding the parameter values that maximize the likelihood of making the observations given the parameters.\n",
    "\n",
    "The method of maximum likelihood corresponds to many well-known estimation methods in statistics. For example, one may be interested in the heights of adult female penguins, but be unable to measure the height of every single penguin in a population due to cost or time constraints. Assuming that the heights are normally distributed with some unknown mean and variance, the mean and variance can be estimated with MLE while only knowing the heights of some sample of the overall population. MLE would accomplish this by taking the mean and variance as parameters and finding particular parametric values that make the observed results the most probable given the model.\n",
    "\n",
    "MLE has a lot of application in linear models and generalized linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minkowski distance  \n",
    "\n",
    "The [Minkowski distance](https://en.wikipedia.org/wiki/Minkowski_distance) is a metric of distance of order p between two points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\ X = (x_1, x_2,..., x_n) \\text{ and } Y = (y_1, y_2,..., y_n) \\in\\mathbb{R}^n\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\large\\left( \\sum_{i=1}^n\\mid x_i-y_i \\mid^p\\right)^{1/p}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minkowski distance is typically used with p being 1 or 2. The latter is the Euclidean distance, while the former is sometimes known as the Manhattan distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misclassifcation rate\n",
    "\n",
    "In a classificaion problem, the misclassification rate measure the percentage of all observations that were incorrectly classified: (FP+FN)/(FP+FN+TP+TN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE\n",
    "\n",
    "[Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error) measures the average of the squares of the errors or deviations—that is, the difference between the estimator and what is estimated. MSE is commonly used to assess the quality of an estimator or a predictor. Mathematically, MSE is defined as:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/67b9ac7353c6a2710e35180238efe54faf4d9c15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested cross-validation\n",
    "\n",
    "[Nested cross-validation (CV)](http://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html) is often used to train a model in which hyperparameters also need to be optimized. Nested CV estimates the generalization error of the underlying model and its (hyper)parameter search. Choosing the parameters that maximize non-nested CV biases the model to the dataset, yielding an overly-optimistic score.\n",
    "\n",
    "Model selection without nested CV uses the same data to tune model parameters and evaluate model performance. Information may thus “leak” into the model and overfit the data. The magnitude of this effect is primarily dependent on the size of the dataset and the stability of the model.\n",
    "\n",
    "To avoid this problem, nested CV effectively uses a series of train/validation/test set splits. In the inner loop, the score is approximately maximized by fitting a model to each training set, and then directly maximized in selecting (hyper)parameters over the validation set. In the outer loop, generalization error is estimated by averaging test set scores over several dataset splits.\n",
    "\n",
    "For more, see corresponding [module](https://github.com/zlatankr/Data-Science-Curriculum/blob/master/Cross-Validation.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nominal vs ordinal features\n",
    "In the context of categorical features, nominal values or observations are ones that can be assigned a code in the form of a number where the numbers are simply labels (they can be counted but not ordered). Ordinal values or observations can be ranked or have a rating scale attached. We can count _and_ order ordinal features, but not measure them. Depending on the situation, ordinal variables can be treated as categorical or as quantitative variables. Nominal values are not to be confused with cardinal numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonparametric models\n",
    "Nonparametric models are algorithms that do not make strong assumptions about the form of the mapping function. The parameters are usually said to be infinite in dimensions and so can express the characteristics in the data much better than parametric models. In the non-parametric approach, all of the previously used training data needs to be used along with the untrained values and the estimated quantities in order to perform a prediction. Examples include k-nearest neighbors and support vector machines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "In feature scaling, <a href=\"https://en.wikipedia.org/wiki/Normalization_(statistics)\">normalization</a> means adjusting values measured on different scales to a notionally common scale. In many contexts, normalization is a technique used to bring all values into the range [0,1]. This is also called unity-based normalization.\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/0222c9472478eec2857b8bcbfa4148ece4a11b84)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function\n",
    "\n",
    "An [objective function](https://en.wikipedia.org/wiki/Loss_function) (also known as a loss function or its negative), is a function that maps an event or values of one or more variables onto a real number, intuitively representing some \"cost\" associated with the event. An optimization problem seeks to maximize the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odds ratio\n",
    "\n",
    "An [odds ratio](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2938757/) (OR) is a measure of association between an exposure and an outcome. The OR represents the odds that an outcome will occur given a particular exposure, compared to the odds of the outcome occurring in the absence of that exposure. Odds ratios are most commonly used in case-control studies, however they can also be used in cross-sectional and cohort study designs as well (with some modifications and/or assumptions).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\text{Odds ratio } = \\frac{PG_1/(1-PG_1)}{PG_2/(1-PG_2)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where “PG1” represents the odds of the event of interest for Group 1, and “PG2” represents the odds of the event of interest for Group 2.\n",
    "\n",
    "Interpreting the odds ratio:\n",
    "\n",
    "- OR=1 Exposure does not affect odds of outcome  \n",
    "- OR>1 Exposure associated with higher odds of outcome  \n",
    "- OR<1 Exposure associated with lower odds of outcome  \n",
    "\n",
    "The most common significance tests for the odds ratio are the Fisher’s Exact Probability test, the Pearson Chi-Square, and the Likelihood Ratio Chi-Square (http://www.biochemia-medica.com/content/odds-ratio-calculation-usage-and-interpretation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "A process of creating dummy features for each unique value in the nominal feature column (it returns a sparse matrix using the transform method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of core learning\n",
    "\n",
    "Out-of-core or external memory algorithms are algorithms that are designed to process data that is too large to fit into a computer's main memory at one time. Such algorithms must be optimized to efficiently fetch and access data stored in slow bulk memory (auxiliary memory) such as hard drives or tape drives.\n",
    "\n",
    "See Python Machine Learning for examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "If a model is too complex for a given training dataset—there are too many degrees of freedom or parameters in this model—the model tends to overfit the training data and does not generalize well to unseen data. Often, it can help to collect more training samples to reduce the degree of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric models\n",
    "Parametric models assume a finite set of parameters that can be estimated and used to represent the functional form of the underlying data. Examples include logistic and linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson product-moment correlation coefficients  \n",
    "\n",
    "Measures the linear dependence between pairs of features. See correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "Also known as the positive predictive value, precision measures the proportion of predicted positive values that are correct.  \n",
    "PPV = TP/(TP + FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Curves\n",
    "\n",
    "A precision-recall curve compares the precision (y-axis) to the recall (x-axis) of a given algorithm. It is generated by varying the threshold for positive classification for the entire range of predicted probabilities. Although the precision-recall curve is often 'jagged' looking, we can interpolate it using the method below. Choosing the proper threshold from the curve depends on the given problem at hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x87d66a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x81e4240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# taken from http://stackoverflow.com/questions/39836953/how-to-draw-a-precision-recall-curve-with-interpolation-in-python\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "recall = np.linspace(0.0, 1.0, num=42)\n",
    "precision = np.random.rand(42)*(1.-recall)\n",
    "\n",
    "# take a running maximum over the reversed vector of precision values, reverse the\n",
    "# result to match the order of the recall vector\n",
    "decreasing_max_precision = np.maximum.accumulate(precision[::-1])[::-1]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(recall, precision, '--b')\n",
    "ax.step(recall, decreasing_max_precision, '-r')\n",
    "ax.set_xlabel(\"Recall\")\n",
    "ax.set_ylabel(\"Precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor variable\n",
    "\n",
    "Another name for an independent or explanatory variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)  \n",
    "\n",
    "Principal Component Analysis is an unsupervised linear transformation technique used in dimensionality reduction, where it attempts to find the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal of fewer dimensions. See corresponding [module](https://github.com/zlatankr/Data-Science-Curriculum/blob/master/Principal%20Component%20Analysis.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Density Function  \n",
    "\n",
    "The [probability density function](https://en.wikipedia.org/wiki/Probability_density_function) or PDF, is used to specify the probability that a continuous random variable falls within a range of values. The absolute values that the variable would take on a specific value is zero, since there are an infinite possible values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Mass Function  \n",
    "\n",
    "The [probability mass function](https://en.wikipedia.org/wiki/Probability_mass_function) or PMF, is a function that gives a probability that a descrete random variable is exactly equal to some value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype-based clustering\n",
    "\n",
    "Prototype-based clustering means that each cluster is represented by a prototype, which can either be the centroid (average) of similar points with continuous features, or the medoid (the most representative or most frequently occurring point) in the case of categorical features. The k-means algorithm belongs to the category of prototype-based clustering. Two other categories of clustering are hierarchical and density-based clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression  \n",
    "\n",
    "See corresponding [module](https://github.com/zlatankr/Data-Science-Curriculum/blob/master/Decision%20Trees%20and%20Random%20Forest.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sample Consensus (RANSAC) algorithm\n",
    "\n",
    "The [RANdom SAmple Consensus (RANSAC)](https://en.wikipedia.org/wiki/Random_sample_consensus) algorithm fits a regression model to a subset of the data, the so-called inliers.\n",
    "\n",
    "We can summarize the iterative RANSAC algorithm as follows:\n",
    "\n",
    "1. Select a random number of samples to be inliers and fit the model.\n",
    "2. Test all other data points against the fitted model and add those points that fall within a user-given tolerance to the inliers.\n",
    "3. Refit the model using all inliers.\n",
    "4. Estimate the error of the fitted model versus the inliers.\n",
    "5. Terminate the algorithm if the performance meets a certain user-defined threshold or if a fixed number of iterations has been reached; go back to step 1 otherwise.\n",
    "\n",
    "This diagram illustrates the final model of the RANSAC algorithm:\n",
    "\n",
    "![](extras/RANSAC.png)\n",
    "\n",
    "Consult Python Machine Learning for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "Also known as the true positive rate or sensitivity, recall measures the proportion of positives that are correctly identified as such. TPR = TP/P = TP/(TP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operating Characteristic (ROC)\n",
    "\n",
    "A [receiver operating characteristic (ROC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic), or ROC curve, is a graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied. The curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The true-positive rate is also known as sensitivity, recall or probability of detection in machine learning. The false-positive rate is also known as the fall-out or probability of false alarm and can be calculated as (1 − specificity). The ROC curve is thus the sensitivity as a function of fall-out.\n",
    "\n",
    "For more info, see corresponding [module](https://github.com/zlatankr/Data-Science-Curriculum/blob/master/Learning%20Curve%2C%20Validation%20Curve%2C%20and%20ROC.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "The introduction of additional information (bias) to penalize extreme parameter weights (L1 & L2 regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Plots\n",
    "\n",
    "A residual plot is a graph that shows the residuals on the vertical axis and the independent variable (or sometimes the predicted outcome) on the horizontal axis. If the points in a residual plot are randomly dispersed around the horizontal axis, a linear regression model is appropriate for the data; otherwise, a non-linear model is more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response variable\n",
    "Outcome variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restricted Boltzmann Machines  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "Ridge regression is an L2 penalized model where we simply add the squared sum of the weights to our least-squares cost function:\n",
    "\n",
    "![](extras/Ridge1.png)\n",
    "\n",
    "By increasing the value of the hyperparameter λ, we increase the regularization strength and shrink the weights of our model. Note that we don't regularize the intercept term w0 (wait, but do we??).  \n",
    "\n",
    "For more info, see corresponding [module](https://github.com/zlatankr/Data-Science-Curriculum/blob/master/Regularization%20-%20LASSO%20and%20Ridge%20Regression.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection Bias  \n",
    "\n",
    "[Selection bias](https://en.wikipedia.org/wiki/Selection_bias) is the selection of individuals, groups or data for analysis in such a way that proper randomization is not achieved, thereby ensuring that the sample obtained is not representative of the population intended to be analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity\n",
    "Also known as the true positive rate or recall, sensitivity measures the proportion of positives that are correctly identified as such.  \n",
    "TPR = TP/P = TP/(TP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Backward Selection (SBS)\n",
    "A technique of dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function\n",
    "\n",
    "A [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) is a bounded differentiable real function that is defined for all real input values and has a positive derivative at each point. Often, sigmoid function refers to the special case of the logistic function defined by the formula\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/a26a3fa3cbb41a3abfe4c7ff88d47f0181489d13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sillhouette plots\n",
    "\n",
    "A [Silhouette plot](https://en.wikipedia.org/wiki/Silhouette_(clustering%29) is a method of interpretation and validation of consistency within clusters of data. The technique provides a succinct graphical representation of how well each object lies within its cluster. It was first described by Peter J. Rousseeuw in 1986.\n",
    "\n",
    "The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.\n",
    "\n",
    "The silhouette can be calculated with any distance metric, such as the Euclidean distance or the Manhattan distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single linkage\n",
    "\n",
    "[Single-linkage clustering](https://en.wikipedia.org/wiki/Single-linkage_clustering) is one of several methods of hierarchical clustering. It is based on grouping clusters in bottom-up fashion (agglomerative clustering), at each step combining two clusters that contain the closest pair of elements not yet belonging to the same cluster as each other.\n",
    "\n",
    "A drawback of this method is that it tends to produce long thin clusters in which nearby elements of the same cluster have small distances, but elements at opposite ends of a cluster may be much farther from each other than to elements of other clusters. This is known as the chaining effect, and it may lead to difficulties in defining classes that could usefully subdivide the data.\n",
    "\n",
    "![](extras/linkage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition\n",
    "\n",
    "Singular Value Decomposition: In linear algebra, SVD is a factorization of a real complex matrix. For a given m * n matrix M, there exists a decomposition such that M = UΣV, where U and V are unitary matrices and Σ is a diagonal matrix.\n",
    "\n",
    "Singular Value Decomposition\n",
    "PCA is actually a simple application of SVD. In computer vision, the 1st face recognition algorithms used PCA and SVD in order to represent faces as a linear combination of “eigenfaces”, do dimensionality reduction, and then match faces to identities via simple methods; although modern methods are much more sophisticated, many still depend on similar techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft clustering\n",
    "\n",
    "[Fuzzy clustering](https://en.wikipedia.org/wiki/Fuzzy_clustering) (also referred to as soft clustering) is a form of clustering in which each data point can belong to more than one cluster.\n",
    "\n",
    "Clustering or cluster analysis involves assigning data points to clusters (also called buckets, bins, or classes), or homogeneous classes, such that items in the same class or cluster are as similar as possible, while items belonging to different classes are as dissimilar as possible. Clusters are identified via similarity measures. These similarity measures include distance, connectivity, and intensity. Different similarity measures may be chosen based on the data or the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse vector\n",
    "A vector made up of mostly zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specificity\n",
    "Also known as the true negative rate, specificity measures the proportion of negatives that are correctly identified as such.  \n",
    "SPC = TN/N = TN/(TN+FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation\n",
    "\n",
    "[Standard deviation](https://en.wikipedia.org/wiki/Standard_deviation) is used to quantify the amount of variation or dispersion of a set of data values. A low standard deviation indicates that the data points tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the data points are spread out over a wider range of values. Standard deviation is the square root of variance, and it has the advantage of being in the same units of measure as the random variable. \n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/6d5e98f44504dd173af145ff80cda7cfd80d3050)\n",
    "\n",
    "In the image below, dark blue is one standard deviation on either side of the mean. For the normal distribution, this accounts for 68.27 percent of the set; while two standard deviations from the mean (medium and dark blue) account for 95.45 percent; three standard deviations (light, medium, and dark blue) account for 99.73 percent; and four standard deviations account for 99.994 percent. The two points of the curve that are one standard deviation from the mean are also the inflection points.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Standard_deviation_diagram.svg/350px-Standard_deviation_diagram.svg.png)\n",
    "\n",
    "It is very important to note that the standard deviation of a population and the standard error of a statistic derived from that population (such as the mean) are quite different but related (related by the inverse of the square root of the number of observations). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Error  (know the difference between standard error and standard deviation)  \n",
    "\n",
    "http://stattrek.com/estimation/standard-error.aspx?tutorial=ap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "[Feature standardization](https://en.wikipedia.org/wiki/Feature_scaling#Standardization) makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance. This method is widely used for normalization in many machine learning algorithms (e.g., support vector machines, logistic regression, and neural networks).\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/b0aa2e7d203db1526c577192f2d9102b718eafd5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "[Stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (often shortened in SGD), also known as incremental gradient descent, is a stochastic approximation of the gradient descent optimization method for minimizing an objective function that is written as a sum of differentiable functions. In other words, SGD tries to find minimums or maximums by iteration.\n",
    "\n",
    "Both statistical estimation and machine learning consider the problem of minimizing an objective function that has the form of a sum:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/7f38e0bdfea090cfd651222e7db9806dce6164cd)\n",
    "\n",
    "where the parameter w which minimizes Q(w) is to be estimated. Each summand function Q(i) is typically associated with the i-th observation in the data set (used for training).\n",
    "\n",
    "When used to minimize the above function, a standard (or \"batch\") gradient descent method would perform the following iterations:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/a8e5df1aee26f4a46d57443becc517be1b50a241)\n",
    "\n",
    "where eta is a step size (sometimes called the learning rate in machine learning), and the upside down triangle is the gradient of the cost function.\n",
    "\n",
    "In many cases, the summand functions have a simple form that enables inexpensive evaluations of the sum-function and the sum gradient. For example, in statistics, one-parameter exponential families allow economical function-evaluations and gradient-evaluations.\n",
    "\n",
    "However, in other cases, evaluating the sum-gradient may require expensive evaluations of the gradients from all summand functions. When the training set is enormous and no simple formulas exist, evaluating the sums of gradients becomes very expensive, because evaluating the gradient requires evaluating all the summand functions' gradients. To economize on the computational cost at every iteration, stochastic gradient descent samples a subset of summand functions at every step. This is very effective in the case of large-scale machine learning problems.\n",
    "\n",
    "In stochastic (or \"on-line\") gradient descent, the true gradient of Q(w) is approximated by a gradient at a single example:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/dda3670f8a8996a0d3bf80856bb4a166cc8db6d4)\n",
    "\n",
    "As the algorithm sweeps through the training set, it performs the above update for each training example. Several passes can be made over the training set until the algorithm converges. If this is done, the data can be shuffled for each pass to prevent cycles. Typical implementations may use an adaptive learning rate so that the algorithm converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop-words\n",
    "\n",
    "In computing, [stop words](https://en.wikipedia.org/wiki/Stop_words) are words which are filtered out before or after processing of natural language data (text). Though stop words usually refer to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. For some search engines, they are some of the most common, short function words, such as _the_, _is_, _at_, _which_, and _on_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified cross-validation\n",
    "\n",
    "In [stratified k-fold cross-validation](http://stats.stackexchange.com/questions/49540/understanding-stratified-cross-validation), the folds are selected so that the mean response value is approximately equal in all the folds. In the case of a dichotomous classification, this means that each fold contains roughly the same proportions of the two types of class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum of Squared Errors (SSE)\n",
    "\n",
    "In statistics, the [residual sum of squares (RSS)](https://en.wikipedia.org/wiki/Residual_sum_of_squares), also known as the sum of squared residuals (SSR) or the sum of squared errors of prediction (SSE), is the sum of the squares of residuals (deviations predicted from actual empirical values of data). It is a measure of the discrepancy between the data and an estimation model. A small RSS indicates a tight fit of the model to the data. It is used as an optimality criterion in parameter selection and model selection.\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/2f6526aa487b4dc460792bf1eeee79b2bba77709)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency\n",
    "\n",
    "Term frequency is the number of times a term occurs in a document. It is the first of the [tf-idf](https://en.wikipedia.org/wiki/Tf-idf) calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Driven Development  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Negative Rate\n",
    "Also known as specificity, the true negative rate measures the proportion of negatives that are correctly identified as such.  \n",
    "SPC = TN/N = TN/(TN+FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positive Rate\n",
    "Also known as the sensitivity or recall, sensitivity measures the proportion of positives that are correctly identified as such.  \n",
    "TPR = TP/P = TP/(TP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB1\n",
    "\n",
    "The UCB1 algorithm is a solution to the [multi-armed bandit](https://en.wikipedia.org/wiki/Multi-armed_bandit) problem where the expected loss decreases as the N increases. It relies on the Chernoff-Hoeffding bound, which says that for a given value of epsilon greater than zero, this is true:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\ P(\\hat\\mu > \\mu + \\epsilon) \\leq exp(-2\\epsilon^2N)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\epsilon_j = \\sqrt{\\frac{2lnN}{N_j}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where N is the total number of games played so far, and Nj is the total number of times we played arm j. The pseudo code can be written as:  \n",
    "\n",
    "![](extras/ucb1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term in the argmax equation is the estimated mean so far (for example, a click-through rate), where, if it is high enough, we will choose to exploit more often. The second term depends on N and Nj; so if the overall N is high compared to a given j, then we will be tempted to explore the j with the smaller N. However, as N approaches infinity, ln(N)/N approaches zero, so we only use the mean as our decision variable.  \n",
    "\n",
    "More detail here: https://www.chrisstucchio.com/blog/2012/bandit_algorithms_vs_ab.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Curve\n",
    "\n",
    "The validation curve measures the impact of given parameters, such as regularization, on the test and training accuracy in order to monitor performance and overfitting. See relevant [module](https://github.com/zlatankr/Data-Science-Curriculum/blob/master/Learning%20Curve%2C%20Validation%20Curve%2C%20and%20ROC.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Gradient Problem  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance \n",
    "\n",
    "Variance (sigma squared) is the average squared difference between a random variable and its mean value. The larger the variance of a random variable, the more spread out the values of the random variable are. \n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/55622d2a1cf5e46f2926ab389a8e3438edb53731)\n",
    "\n",
    "Alternatively:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/86060a001d3617ca85ed73efb63ec7908d77fa5f)\n",
    "\n",
    "Variance can also be thought of as the covariance of a variable with itself:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/a6df8498bba86383d7d165590f515b929746f243)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ward's Linkage\n",
    "\n",
    "In statistics, [Ward's method or (Ward's linkage)](https://en.wikipedia.org/wiki/Ward's_method) is a criterion applied in hierarchical cluster analysis. Ward's minimum variance method inaccurate, see talk is a special case of the objective function approach originally presented by Joe H. Ward, Jr. Ward's minimum variance criterion minimizes the total within-cluster variance. To implement this method, at each step find the pair of clusters that leads to minimum increase in total within-cluster variance after merging. This increase is a weighted squared distance between cluster centers. At the initial step, all clusters are singletons (clusters containing a single point). To apply a recursive algorithm under this objective function, the initial distance between individual objects must be (proportional to) squared Euclidean distance.\n",
    "\n",
    "The initial cluster distances in Ward's minimum variance method are therefore defined to be the squared Euclidean distance between points:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/b78019e0921865b8b86a43aecf6677dfc997e3c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak learners\n",
    "A simple base classifier, such as a tree stump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word stemming\n",
    "\n",
    "In linguistic morphology and information retrieval, [stemming](https://en.wikipedia.org/wiki/Stemming) is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root.\n",
    "\n",
    "A stemming algorithm reduces the words \"fishing\", \"fished\", and \"fisher\" to the root word, \"fish\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec\n",
    "\n",
    "[Word2vec](https://en.wikipedia.org/wiki/Word2vec) is a group of related text classification models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
