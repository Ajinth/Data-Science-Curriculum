{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear regression allows us model and predict the behavior of continuous variables. In this module, we will study and look at examples of the ordinary least squares method for estimating the parameters in a linear regression model. \n",
    "\n",
    "## Ordinary Least Squares Simple Linear Regression\n",
    "\n",
    "The simple linear regression model is given by\n",
    "![alt](extras/lm4.png)\n",
    "where y is the dependent variable, x is the independent variable, e is the random error term, and B1 and B2 are the regression parameters. \n",
    "\n",
    "Assumptions of a simple linear regression model: \n",
    "1. The mean value of y, for each value of x, is given by the linear regression function:\n",
    "![alt](extras/lm1.png)\n",
    "2. For each value of x, the values of y are distributed about theim mean value, following probability distributions that all have the same variance, \n",
    "![alt](extras/lm2.png)\n",
    "3. The sample value values of y are all uncorrelated and have zero covariance, implying that there is no linear association among them, \n",
    "![alt](extras/lm3.png)\n",
    "This assumption can be made stronger by assuming that the values of y are all statistically independent. \n",
    "\n",
    "4. The variable x is not random and must take at least two different values. \n",
    "\n",
    "5. The values of y are normally disstributed about their mean for ach value of x (optional).\n",
    "6. The value of y, for each value of x, is\n",
    "![alt](extras/lm4.png)\n",
    "7. The expected value of the random error e is \n",
    "![alt](extras/lm5.png)\n",
    "which is equivalent to assuming that \n",
    "![alt](extras/lm6.png)\n",
    "8. The variance of the random error e is\n",
    "![alt](extras/lm7.png)\n",
    "The random variables y and e have the same variance because they differ only by a constant. \n",
    "9. The covariance between any pair of random errors ei and ej is\n",
    "![alt](extras/lm8.png)\n",
    "The stronger version of this assumption is that the random errors e are statistically independent, in which case the values of the dependen variable y are also statistically indendent. \n",
    "10. The variable x is not random and must take on at least 2 different values. \n",
    "11. The values of e are normally distributed about their mean if the values of y are normally distributed, and vice versa. \n",
    "![alt](extras/lm9.png)\n",
    "\n",
    "The least squares model attemps to find the line through our data that minimizes the squared residuals, defined as the difference between the observed and predicted values of our outcome variable, where the residual is\n",
    "![alt](extras/lm11.png)\n",
    "and the sum squared residuals is\n",
    "![alt](extras/lm12.png)\n",
    "\n",
    "The least squares estimators b1 and b2 are \n",
    "![alt](extras/lm13.png)\n",
    "and under assumptions 6-10 above, the Gauss Markov Theorem states that b1 and b2 have the smallest variance of all linear and unbiased estimators of B1 and B2. \n",
    "\n",
    "#### The Normality Assumption\n",
    "Our OLS hypothesis tests and interval estimates for the coefficients rely on the assumption that the errors, and hence the dependent variable y, are normally distributed. Although the tests and interval estimates are valid in large samples regardless of the normal distribution of our errors, we might still find ourselves in a position where we want to find an alternative functional form or transform the dependent variable in order to improve our model. We can test for normality using the Jarque-Bera test, which evaluates the skewness (symmetry) and kurtosis ('peakedness' of the distribution) of the residuals. \n",
    "\n",
    "## Multiple Linear Regression Model\n",
    "\n",
    "The multiple linear regression model can be generalized by:\n",
    "![alt](extras/lm17.png)\n",
    "The main assumptions of the multiple regression model are:\n",
    "![alt](extras/lm18.png)\n",
    "\n",
    "## Model Specification and Evaluation\n",
    "Choosing the correct model is a process of combining intuition with empirical observations about the behavior of the model. This section covers the essential tests that can be performed in oder to make sure that we have chosen the correct variables and have the optimal model. \n",
    "\n",
    "### t-test\n",
    "The t-statistic and p-value columns are testing whether any of the coefficients might be equal to zero. The t-statistic is calculated simply as \n",
    "![alt](https://wikimedia.org/api/rest_v1/media/math/render/svg/706d1c514396be8e7301a23ab369cdcf5b1c5096)\n",
    "If the errors ε follow a normal distribution, t follows a Student-t distribution. Under weaker conditions, t is asymptotically normal. Large values of t indicate that the null hypothesis can be rejected and that the corresponding coefficient is not zero. The second column, p-value, expresses the results of the hypothesis test as a significance level. Conventionally, p-values smaller than 0.05 are taken as evidence that the population coefficient is nonzero.\n",
    "\n",
    "### F-Statistic\n",
    "F-statistic tries to test the hypothesis that all coefficients (except the intercept) are equal to zero. This statistic has F(p–1,n–p) distribution under the null hypothesis and normality assumption, and its p-value indicates probability that the hypothesis is indeed true. Note that when errors are not normal this statistic becomes invalid, and other tests such as for example Wald test or LR test should be used.\n",
    "\n",
    "### Measuring goodness of fit  \n",
    "In order understand how much of the variation in the observed value y is explained by the predicted value of yhat, we need to define the total sum of squares (SST) as the sum of squares due to the regression (SSR) plus the sum of squares due to error (SSE):\n",
    "![alt](extras/lm14.png)\n",
    "which becomes\n",
    "![alt](extras/lm15.png)\n",
    "We can now define a measure called the coefficient of determination, or R^2, which is the proportion of variation in y explained by x within the regression model:\n",
    "![alt](extras/lm16.png)\n",
    "\n",
    "### Adjusted R-Squared\n",
    "\n",
    "Adjusted R-squared is a slightly modified version of R^2, designed to penalize for the excess number of regressors which do not add to the explanatory power of the regression. This statistic is always smaller than R^2, can decrease as new regressors are added, and even be negative for poorly fitting models:\n",
    "![alt](https://wikimedia.org/api/rest_v1/media/math/render/svg/7ec4559807623b855036fce5201f9e8b6c7aca4b)\n",
    "\n",
    "### Omitted Variables\n",
    "Omitting an important explanatory variable from our equation results in biased coefficient estimates, but a reduced variance. However, this is only true if the sample covariance between the omitted and the remaining variable is not zero. If the covariance is zero, then the least squares estimator in the misspecified model is still unbiased. \n",
    "\n",
    "### Irrelevant Variables\n",
    "The inclusion of irrelevant variables (often identified by large p-values) can result in increased standard errors of the coefficients estimated for all the variables in the model (thus increasing individual p-values). It also results in a reduced precision of the estimated coefficients for the relevant variables in the equation.\n",
    "\n",
    "### Collinearity\n",
    "When two variables move together in systematic ways, they are said to be collinear, and the problem is labeled as collinearity. When two variables are perfectly correlated, then we have exact or extreme collinearity. Collinearity results in large variance of the estimator, which means a large standard error, which in turn means the estimate may not be significantly different from zero and the interval estimate will be wide. However, it is important to know that non-exact collinearity is not an assumption of the least squares model. Additionally, even though collinearity can make it difficult to isolate the effects of the individual variables, accurate forecasts may still be possible if the nature of the collinear relationship remains the same within the out-of-sample observations.  \n",
    "\n",
    "To test for collinearity, one can look at the correlation coefficients between pairs of explanatory variables. In the cases where collinear relationships involve more than two explanatory variables, we can estimate the 'auxillary regressions,' where the left-hand side is one of the explanatory variables and the right-hand side variables are the remaining explanatory variables: \n",
    "![alt](extras/lm19.png)\n",
    "If the R^2 form this artificial model is high - above .80 say - the implication is that a large portion of the variation in x2 is explained by variation in the other explanatory variables, meaning the precision of b2 is likely negatively affected by this collinearity. \n",
    "\n",
    "One way to reduce the negative effects of collinearity is to collect more, and better sample data. This will give the model more 'information' and allow it to estimate the parameters more precisely. The other method is to use non-sample data in the form of linear constraints on the parameter (such as a constraint that all the parameters have to add up to 1). However, although this method reduces estimator sampling variability, it also increases the estimator bias, unless the constraints are exactly true. \n",
    "\n",
    "### Heteroskedasticity\n",
    "One assumption of the fitted model (to ensure that the least-squares estimators are each a best linear unbiased estimator of the respective population parameters, by the Gauss–Markov theorem) is that the standard deviations of the error terms are constant and do not depend on the x-value. Consequently, each probability distribution for y has the same standard deviation regardless of the x-value (predictor). In short, this assumption is homoscedasticity. Homoscedasticity is not required for the estimates to be unbiased, consistent, and asymptotically normal. While the ordinary least squares estimator is still unbiased in the presence of heteroscedasticity, it is inefficient because the true variance and covariance are underestimated. Biased standard errors lead to biased inference, so results of hypothesis tests are possibly wrong.\n",
    "\n",
    "One can visually identify heteroskedasticity by plotting the residuals against the independent variable (in the case of simple linear regression) or against the fitted values (yhat). The most popular statistical methods for detecting heteroskedasticity are the White Test or the Breusch-Pagan (Lagrange multiplier) test. \n",
    "\n",
    "There are several common corrections for heteroscedasticity. They are:\n",
    "- View logarithmized data. Non-logarithmized series that are growing exponentially often appear to have increasing variability, random volatility, or volatility clusters as the series rises over time. The variability in percentage terms may, however, be rather stable. The reason for this is that the likelihood function for exponentially growing data lacks a variance. Using regression, the maximum likelihood estimator is the least squares estimator, a form of the sample mean, but the sampling distribution of the estimator is the Cauchy distribution. The Cauchy distribution has no variance and so there is no fixed point for the sample variance to converge to causing it to behave as a random number. Taking the logarithm of the data converts the likelihood function to the hyperbolic secant distribution, which has a defined variance.\n",
    "- Use a different specification for the model (different X variables, or perhaps non-linear transformations of the X variables).\n",
    "- Apply a weighted least squares estimation method, in which OLS is applied to transformed or weighted values of X and Y. The weights vary over observations, usually depending on the changing error variances. In one variation the weights are directly related to the magnitude of the dependent variable, and this corresponds to least squares percentage regression.\n",
    "\n",
    "### Testing Nested Models using the F-Test\n",
    "- Two models are nested if both contain the same terms and one has at\n",
    "least one additional term.\n",
    "- Example:  \n",
    "y = β0 + β1x1 + β2x2 + β3x1x2 + e (1)  \n",
    "y = β0 + β1x1 + β2x2 + β3x1x2 + β4x + β5x + e (2)  \n",
    "- Model (1) is nested within model (2).\n",
    "- Model (1) is the reduced model and model (2) is the full model.\n",
    "- How do we decide whether the more complex (full) model contributes additional information about the association between y and the predictors?\n",
    "- In example above, this is equivalent to testing H0 : β4 = β5 = 0 versus Ha : at least one β 6= 0.\n",
    "- Test consists in comparing the SSE for the reduced model (SSER) and the SSE for the complete model (SSEC).\n",
    "- SSER > SSEC always so question is whether the drop in SSE from fitting the complete model is ‘large enough’.\n",
    "- We use an F−test to compare nested models, one with k parameters (reduced) and another one with k + p parameters (complete or full).\n",
    "- Hypotheses: H0 : βk+1 = βk+2 = ... = βk+p = 0 versus Ha : At least one β 6= 0.\n",
    "- Test statistic: F = ((SSER−SSEC)/ # of additional βs)/(SSEC/[n−(k+p+1)])\n",
    "- At level α, we compare the F−statistic to an Fν1,ν2 from table, where ν1 = p and ν2 = n − (k + p + 1).\n",
    "- If F ≥ Fα,ν1,ν2, reject H0.\n",
    "\n",
    "### Testing Non-Nested Models\n",
    "To compare non-nested models, we can use the Cox test, the Davidson-MacKinnon J test, or theencompassing test of Davidson & MacKinnon."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
